# Imladris 2.0 - Project Specification

> A reproducible Linux cloud workstation that captures all inputs from life and work, surfaces actionable items, organizes by context (workspaces), and provides frictionless AI-assisted tools to act on them.

**Version:** 0.3.0
**Last Updated:** 2026-02-17

---

## Table of Contents

1. [Overview](#1-overview)
2. [Guiding Principles](#2-guiding-principles)
3. [Architecture](#3-architecture)
4. [Workspaces](#4-workspaces)
5. [Datahub](#5-datahub)
6. [Authentication](#6-authentication)
7. [Commands](#7-commands)
8. [Infrastructure](#8-infrastructure)
9. [User Scenarios](#9-user-scenarios)
10. [Coding Methodology](#10-coding-methodology)
11. [Mobile Access (OpenClaw)](#11-mobile-access-openclaw)
12. [Out of Scope](#12-out-of-scope)
13. [Open Questions](#13-open-questions)

---

## 1. Overview

### 1.1 Purpose

Imladris 2.0 is a personal cloud workstation for:
- Running Claude Code with PAI for personal productivity
- AI-assisted development environment for work projects
- Aggregating all life and work inputs into a unified system
- Research assistance

### 1.2 Core Components

| Component | Purpose |
|-----------|---------|
| **Workspaces** | Zone + mode context organization |
| **Datahub** | Capture all inputs, sync bidirectionally |
| **Triage** | Surface actionable items via Claude |
| **Tools** | PAI + Claude Code + skills |
| **Auth** | Frictionless credentials via BWS |

### 1.3 Boundaries

**In Scope:**
- Collector and workspace
- Aggregates and presents
- User decides and acts

**Out of Scope:**
- Outbound automation (bots replying, scheduled actions)
- Multi-instance / HA / scaling
- Building a task/project management system

---

## 2. Guiding Principles

### Principle 1: Context Switching is Expensive
- 23 minutes average recovery time after interruption
- Zone switches cost more than mode switches
- Design: Make zone switches deliberate, mode switches easy

### Principle 2: Modes Map to Cognitive Action Types
| Mode | Action Type |
|------|-------------|
| tasks | Execute |
| comms | Communicate |
| projects | Create |
| research | Learn |
| adhoc | Quick/flexible |

### Principle 3: Interstitial Journaling on Transitions
- Claude auto-summarizes context on zone switch
- Saves 23+ minutes of "where was I?" on return

### Principle 4: Friction is Intentional
| Transition | Friction Level | What This Means |
|------------|----------------|-----------------|
| Start working | Low | Just `/work`, context loads automatically |
| Within mode | Low | Commands execute immediately |
| Mode â†’ Mode (same zone) | Medium | Brief pause, context summary shown |
| Zone â†’ Zone | Higher | Full context save, confirmation prompt |

### Principle 5: Deep vs Shallow Work Separation
| Shallow (batch) | Deep (protect) |
|-----------------|----------------|
| tasks, comms, adhoc | projects, research |

### Principle 6: Working Memory Limits (3-5 Items)
- 5 modes is optimal
- 2 zones is manageable
- 10 total workspaces within bounds

### Principle 7: Plain Text Everything
- Greppable, scriptable, version-controlled, composable, portable

### Principle 8: Persistence Over Memory
- Session state: tmux
- Context notes: Claude auto-save
- Tasks: flat files
- Inputs: datahub
- Auth: lazy-loaded, auto-refreshed

### Principle 9: Defaults Over Decisions
- Workspace entry has sensible defaults
- Mode switching requires minimal thought
- "Just start" always possible

### Principle 10: Visual Cues = Cognitive Offloading
- Prompt prefix shows workspace
- Status bar shows context
- Colors indicate zone

### Principle 11: Batch Shallow Work
- comms collects all communication
- tasks collects all actionable items
- Process in windows, then close

### Principle 12: Zone Entry = Room Entry
- Switching zones feels deliberate
- Clear separation of concerns
- Work stays in work, home stays in home

---

## 3. Architecture

### 3.1 System Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  imladris 2.0                                                       â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Workspaces (tmux)                                            â”‚   â”‚
â”‚  â”‚  Window 0:  status (TUI dashboard)                           â”‚   â”‚
â”‚  â”‚  Window 1-5:  work:comms/tasks/projects/research/adhoc       â”‚   â”‚
â”‚  â”‚  Window 6-10: home:comms/tasks/projects/research/adhoc       â”‚   â”‚
â”‚  â”‚                                                               â”‚   â”‚
â”‚  â”‚  Each window: Claude Code session (PAI)                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                      â”‚
â”‚                   Curu skills (PAI) call Windmill API                      â”‚
â”‚                              â”‚                                      â”‚
â”‚                              â–¼                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Windmill (native via Nix/systemd) â—„â”€â”€ Integration Gateway    â”‚   â”‚
â”‚  â”‚                                                               â”‚   â”‚
â”‚  â”‚  Scheduled:  pollers/* (sync data â†’ datahub, 60s-15min)      â”‚   â”‚
â”‚  â”‚  On-demand:  queries/* (adhoc lookups, triggered by PAI)     â”‚   â”‚
â”‚  â”‚                                                               â”‚   â”‚
â”‚  â”‚  ALL external API calls go through Windmill scripts          â”‚   â”‚
â”‚  â”‚  Credentials: Windmill variables (synced from BWS)           â”‚   â”‚
â”‚  â”‚  Built-in: retries, rate limits, logging, audit              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                      â”‚
â”‚                              â–¼                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Datahub                                                      â”‚   â”‚
â”‚  â”‚  ~/work/datahub/items/    (flat files, source of truth)      â”‚   â”‚
â”‚  â”‚  ~/work/datahub/index.sqlite (derived, fast queries)         â”‚   â”‚
â”‚  â”‚  ~/home/datahub/items/                                       â”‚   â”‚
â”‚  â”‚  ~/home/datahub/index.sqlite                                 â”‚   â”‚
â”‚  â”‚  ~/calendar/merged.sqlite (read-only combined view)          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ Tailscale VPN (no public ports)
                              â–¼
    External Systems: SDP, MS365, DevOps, Slack, Gmail, GCal,
                      Telegram, Ramp, Securonix, etc.
```

### 3.2 Technology Stack

| Layer | Technology |
|-------|------------|
| Infrastructure | Terraform (AWS) |
| Configuration | Nix + home-manager |
| Integration Gateway | Windmill (APIs, credentials, scheduling, monitoring) |
| Runtime | Bun/Python (scripts in Windmill) |
| AI | Claude Code via AWS Bedrock |
| Framework | PAI (Personal AI Infrastructure) |
| Storage | Flat files (markdown) + SQLite index |
| Secrets | BWS (bootstrap only â†’ Windmill variables) |
| Access | Tailscale SSH only |

### 3.3 Simplified Component Model

**Only 4 systemd services:**

| Service | Purpose |
|---------|---------|
| `postgresql` | Windmill job queue |
| `windmill-server` | API + UI |
| `windmill-worker` | Script execution |
| `tailscaled` | Network access |

Everything else runs in Windmill.

**Eliminated components:**

| Removed | Replaced By |
|---------|-------------|
| auth-keeper.sh (~1800 lines) | Windmill OAuth resources |
| Custom queue processor | Windmill job queue |
| systemd timers (app-level) | Windmill schedules |
| Per-service Curu skills (PAI) | Single Windmill skill |
| Custom status/monitoring | Windmill UI |
| Scattered retry logic | Windmill policies |

**What remains:**

```
Bootstrap:
  systemd â†’ postgres â†’ windmill â†’ tailscale
  luks-unlock (oneshot, pre-boot)
  bws-to-windmill.ts (one-time secret sync)

Runtime:
  Windmill handles everything else

User Interface:
  tmux + workspaces
  Claude Code + PAI
  One Windmill skill (routes all external calls)
```

### 3.4 Data Flow

```
                    External Systems
                          â–²
                          â”‚
                          â”‚ ALL external API calls
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Windmill (Gateway)                  â”‚
â”‚                                                  â”‚
â”‚   Inbound:   scheduled pollers â†’ datahub        â”‚
â”‚              (60s chat, 5min email/tickets)     â”‚
â”‚   Outbound:  on-demand scripts â† PAI triggers   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼                           â–¼
     Datahub (files)            Direct responses
            â”‚                   (adhoc queries)
            â–¼
     Triage (Claude)
            â”‚
            â–¼
     Index (SQLite)
            â”‚
            â–¼
     Workspaces (Claude sessions)
            â”‚
            â”‚ Curu skill (PAI) triggers Windmill
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
```

**Key principle:** Claude/PAI never calls external APIs directly. All external communication routes through Windmill scripts.

#### Detailed Data Flow (Mermaid)

```mermaid
graph TD
    subgraph External [External World]
        API[Cloud APIs<br/>SDP, MS365, Slack]
    end

    subgraph Gateway [Windmill Layer]
        Pollers[Scheduled Pollers<br/>f/*/sync.ts<br/>60s chat, 5min email]
        Out_Job[Outbound Jobs<br/>f/*/update-*.ts]
    end

    subgraph Host [Imladris Host]
        DH[Datahub CLI<br/>dh]
        Files[(Flat Files<br/>/data/work/datahub)]
        Lock{flock}

        subgraph Workspace [Tmux Session]
            Claude[Claude Code + PAI]
        end
        OpenClaw[OpenClaw<br/>Mobile Access]
    end

    %% Inbound Flow (polling only)
    API -->|Poll| Pollers
    Pollers --> DH
    DH --> Lock
    Lock --> Files

    %% Interaction Flow
    Files -->|Read Context| Claude
    Claude -->|Write Context| Lock
    Claude -->|Commands| DH
    OpenClaw -->|Read Context| Files

    %% Outbound Flow
    DH -->|Queue Item| Out_Job
    Out_Job -->|API Call| API
```

**Key components:**

| Component | Role |
|-----------|------|
| Windmill | Gateway for all external API calls |
| `dh` CLI | Single interface for datahub operations (used by humans + automation) |
| `flock` | Prevents race conditions in parallel pane scenarios |
| Flat files | Source of truth (greppable, portable) |

### 3.5 PAI Alignment

Imladris is built **on top of PAI** (Personal AI Infrastructure), not alongside it. We use PAI's systems, not parallel implementations.

**PAI Systems We Use:**

| PAI System | How Imladris Uses It |
|------------|---------------------|
| **TELOS** | Zone-specific goals stored in GOALS.md, PROJECTS.md |
| **Memory (Hot/Warm/Cold)** | Context save/restore uses PAI's 3-tier memory |
| **Hook System** | Event handling via PAI's 20 hooks across 6 lifecycle events |
| **Skill System** | Curu skills follow PAI skill format |
| **Security System** | PAI hooks validate commands before execution |
| **User/System Separation** | Imladris configs in USER/, upgrades don't break |

**PAI Principles We Follow:**

| Principle | Application |
|-----------|-------------|
| Scaffolding > Model | Thread-based triage with good context beats model upgrades |
| Code Before Prompts | Windmill scripts are deterministic where possible |
| Deterministic Infrastructure | Templates, patterns, flat files over probabilistic |
| Algorithm v1.4.0 | 7-phase (Observeâ†’Thinkâ†’Planâ†’Buildâ†’Executeâ†’Verifyâ†’Learn) + ISC + drift prevention |

**What Imladris Adds (Not in PAI):**

| Addition | Purpose |
|----------|---------|
| Datahub | Multi-source message aggregation |
| Thread-based triage | Conversation context for classification |
| Workspaces/Zones | work/home separation in tmux |
| Windmill | Scheduled polling, credential management |
| Bidirectional sync | Write-back to external systems |

#### Architectural Clarifications

**What Windmill IS:**
- Integration gateway for external APIs (SDP, MS365, Slack, Gmail, etc.)
- Scheduled poller running `f/{source}/sync.ts` scripts
- Credential manager (OAuth refresh, API keys via variables)
- On-demand query executor triggered by PAI skills

**What Windmill is NOT:**
- NOT an orchestrator for Claude Code
- NOT driving Claude via tmux send-keys
- NOT spawning background AI agents
- NOT a replacement for humanâ†”Claude interaction

**The actual interaction flow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Human types in Claude Code (tmux workspace)                    â”‚
â”‚      â†“                                                          â”‚
â”‚  Claude reasons, decides to fetch external data                 â”‚
â”‚      â†“                                                          â”‚
â”‚  PAI skill calls Windmill API: "get my SDP tickets"             â”‚
â”‚      â†“                                                          â”‚
â”‚  Windmill executes f/sdp/get-tickets.ts                         â”‚
â”‚      â†“                                                          â”‚
â”‚  Results return to Claude via skill                             â”‚
â”‚      â†“                                                          â”‚
â”‚  Claude continues conversation with human                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Common misconception:** "Windmill orchestrates Claude Code via tmux"

This is incorrect. The core workflow is always humanâ†”Claude interactive via tmux. Mobile access (Section 11) uses OpenClaw with its own Claude API connection, not tmux automation.

| Component | Interaction Pattern |
|-----------|---------------------|
| Human â†” Claude | Direct typing in tmux (interactive, primary) |
| Claude â†’ Windmill | API calls via PAI skills (request/response) |
| Windmill â†’ External | Scheduled polling + on-demand queries (deterministic) |
| OpenClaw â†’ Claude | Separate API connection (mobile, secondary) |

**Why this matters:**
- No "terminal scraping" anywhere in the system
- Windmill jobs are finite and deterministic
- Claude Code runs interactively, not as a Windmill job
- OpenClaw uses clean API access, not tmux send-keys

**Future-proofing:** When PAI adds new features (e.g., Granular Model Routing), imladris will adopt them rather than maintaining parallel implementations.

#### Layer Responsibilities (Simplified)

Each layer has ONE job. No overlap.

| Layer | Responsibility | Does NOT Do |
|-------|----------------|-------------|
| **PAI v3.0** | Algorithm, skills, MEMORY, PRDs, agent teams | API calls, auth, scheduling |
| **Imladris** | Context injection, zone awareness | API logic, auth, thinking |
| **Windmill** | Auth, API calls, scheduling, credentials | Thinking, triage decisions, orchestrating Claude |
| **Datahub** | Unified data storage, queryable index | API calls, auth, thinking |

```
User asks: "What's my top priority today?"
     â”‚
     â–¼
PAI thinks (using Algorithm, skills, context)
     â”‚
     â–¼
PAI decides it needs data â†’ calls Windmill script
     â”‚
     â–¼
Windmill handles auth, calls SDP API, returns data
     â”‚
     â–¼
PAI continues thinking with data
     â”‚
     â–¼
PAI responds with recommendation
```

**Windmill never decides. Windmill only fetches/sends.**

#### MVP Integrations (Start Here)

| Folder | Scripts | Auth | Priority |
|--------|---------|------|----------|
| `f/sdp/` | get-tickets, update-ticket | API key | P0 |
| `f/ms365/` | get-mail, get-calendar, send-mail | OAuth | P0 |
| `f/slack/` | get-messages, post-message | OAuth | P0 |
| `f/telegram/` | get-updates, send-message | Bot token | P1 |
| `f/ops/` | backup, health-check | Instance role | P1 |

**Add more integrations only when actually needed.**

### 3.6 PAI Agent Usage (v3.0)

PAI v3.0 provides thinking tools, composition patterns, and agent teams for complex reasoning. Imladris uses these **as PAI intended** - agents are the last resort in PAI's decision hierarchy:

```
Goal â†’ Code â†’ CLI â†’ Prompts â†’ Agents
 â†‘                              â†‘
 Most preferred          Last resort
```

#### PAI v3.0 Key Changes

| Feature | What It Means for Imladris |
|---------|---------------------------|
| **Algorithm v1.4.0** | ISC criteria, constraint extraction, drift prevention |
| **Persistent PRDs** | Task context survives sessions - integrates with datahub |
| **Agent Teams** | Native swarm coordination for large features |
| **Loop Mode** | Unbounded parallel workers for 100+ criteria tasks |
| **20 Hooks** | Full lifecycle coverage (vs 17 in v2.5) |
| **Context Recovery** | â‰¤34s cold-start recovery from MEMORY/PRDs |
| **Build Drift Prevention** | Re-read ISC before each artifact |
| **37 Skills** | +9 new skills (Science, IterativeDepth, Evals, etc.) |

#### Composition Patterns

PAI v3.0 defines 7 named patterns for combining capabilities:

| Pattern | Shape | When Imladris Uses It |
|---------|-------|----------------------|
| **Pipeline** | A â†’ B â†’ C | Sequential domain handoff (Architect â†’ Engineer â†’ QA) |
| **TDD Loop** | A â†” B | Build-verify cycle until criteria pass |
| **Fan-out** | â†’ [A, B, C] | Multiple research perspectives in parallel |
| **Fan-in** | [A, B, C] â†’ D | Merging parallel triage results |
| **Gate** | A â†’ check â†’ B | Quality gate before deployment |
| **Escalation** | haiku â†’ sonnet â†’ opus | Complexity exceeded model tier |
| **Specialist** | Single A | Deep expertise (security review) |

#### Thinking Tools (Justify-Exclusion)

PAI v3.0 inverts the default: thinking tools are **opt-OUT not opt-IN**. For every request, justify why each tool is NOT being used:

| Tool | What It Does | Include When |
|------|-------------|--------------|
| **Council** | Multi-agent debate (3-7 agents) | Multiple valid approaches exist |
| **RedTeam** | Adversarial analysis (32 agents) | Claims need stress-testing |
| **FirstPrinciples** | Deconstruct, challenge, reconstruct | Assumptions need examining |
| **Science** | Hypothesis-test-analyze cycles | Iterative experimentation needed |
| **BeCreative** | Extended thinking, 5 diverse options | Creative divergence needed |
| **Prompting** | Meta-prompting with templates | Prompt optimization needed |

**Example justification block:**

```
ğŸ” THINKING TOOLS ASSESSMENT (justify exclusion):
â”‚ Council:          EXCLUDE â€” single clear approach, no alternatives to debate
â”‚ RedTeam:          INCLUDE â€” Windmill script could fail in non-obvious ways
â”‚ FirstPrinciples:  EXCLUDE â€” requirements are well-defined
â”‚ Science:          EXCLUDE â€” not iterative/experimental
â”‚ BeCreative:       EXCLUDE â€” clear requirements, no divergence needed
â”‚ Prompting:        EXCLUDE â€” not generating prompts
```

#### Available PAI Skill Packs

| PAI Skill Pack | Purpose | When Imladris Uses It |
|----------------|---------|----------------------|
| `pai-agents-skill` | Dynamic agent composition | Open-ended research in `/research` zone |
| `pai-council-skill` | Multi-agent debate (3-7 agents) | Low-confidence triage decisions |
| `pai-redteam-skill` | Adversarial analysis (32 agents) | Security review of Windmill scripts |
| `pai-firstprinciples-skill` | Fundamental analysis | Questioning assumptions in design |
| `pai-research-skill` | Multi-model parallel research | Deep investigation tasks |
| `pai-browser-skill` | Playwright automation | Web verification, testing |

#### Named Agents (PAI v3.0)

PAI v3.0 provides 13 specialized agent personalities with mapped ElevenLabs voices:

| Agent | Specialty | Imladris Usage |
|-------|-----------|----------------|
| **Algorithm** | ISC tracking, verification | Task completion validation |
| **Architect** | System design, distributed systems | Design decisions |
| **Engineer** | TDD, implementation patterns | Code implementation |
| **QATester** | Browser automation, verification | Testing datahub sync |
| **Pentester** | Security testing, vulnerability | Windmill script review |
| **Intern** | High-agency generalist | Quick tasks |
| **Serena Blackwood** | Security architecture | Security-critical decisions |
| **Marcus Webb** | Research synthesis | Deep research tasks |
| **Dev Patel** | Engineering | Feature implementation |
| **PerplexityResearcher** | Web research | External API research |

#### Integration Points

**1. Triage Escalation (pai-council-skill)**

Single-pass triage handles most items. Council only for ambiguous cases:

```
Thread arrives â†’ Single-pass triage â†’ Confidence â‰¥ 70%? â†’ Done
                                            â†“ No
                                     Invoke pai-council-skill:
                                     - Urgency perspective
                                     - Context perspective
                                     - False-positive perspective
                                            â†“
                                     Consensus â†’ Final classification
```

This follows PAI's "Scaffolding > Model" principle - good context (thread-based) handles most cases, agents only when needed.

**2. Research Mode (pai-agents-skill)**

When explicitly entering research, PAI's agent composition helps breadth exploration:

```bash
# In /research zone, explicit invocation
/research "evaluate windmill alternatives"
```

PAI spawns agents per its personality mapping:
- Breadth-researcher: survey options
- Comparison-builder: structured analysis
- Risk-assessor: identify concerns

Results consolidated by PAI, not custom imladris code.

**3. Security Review (pai-redteam-skill)**

Before deploying new Windmill scripts, invoke PAI's adversarial agents:

```bash
/review f/sdp/sync.ts
```

PAI's 32 redteam agents examine:
- Credential handling
- Error propagation
- Rate limit compliance
- Idempotency guarantees

#### What Imladris Does NOT Do

| Anti-pattern | Why Avoided |
|--------------|-------------|
| Custom agent framework | PAI provides agent packs |
| Agents for simple queries | Use direct Windmill calls |
| Agents for deterministic sync | Scripts are sufficient |
| Agents for clear triage | Single-pass handles 70%+ |

#### Invocation Pattern

Agents are invoked through PAI's skill system, not custom imladris code:

```bash
# PAI skill invocation (imladris just calls PAI)
pai council "Should this thread be P1 or P2? Context: {thread_summary}"
pai redteam "Review this script for security issues: {script_content}"
pai agents research "Survey options for {topic}"
```

Imladris provides:
- **Triggers**: Confidence thresholds, explicit commands, zone context
- **Context**: Thread summaries, script content, research topics
- **Integration**: Results flow back to triage DB, commit hooks, research notes

PAI provides:
- **Agent orchestration**: Spawning, coordination, consolidation
- **Personality mapping**: Which agents for which task types
- **Response formatting**: Structured output

This separation ensures PAI upgrades to agent capabilities automatically benefit imladris.

---

## 4. Workspaces

### 4.1 Terminology

| Term | Definition |
|------|------------|
| **Zone** | Top-level context (work, home) |
| **Mode** | Activity type (tasks, comms, projects, research, adhoc) |
| **Workspace** | Zone + mode combination (e.g., work:tasks) |

### 4.2 Window Structure

```
tmux session: main
â”‚
â”œâ”€â”€ window 0:  status        â† TUI dashboard
â”œâ”€â”€ window 1:  work:comms    â† default on /work
â”œâ”€â”€ window 2:  work:tasks
â”œâ”€â”€ window 3:  work:projects
â”œâ”€â”€ window 4:  work:research
â”œâ”€â”€ window 5:  work:adhoc
â”œâ”€â”€ window 6:  home:comms    â† default on /home
â”œâ”€â”€ window 7:  home:tasks
â”œâ”€â”€ window 8:  home:projects
â”œâ”€â”€ window 9:  home:research
â””â”€â”€ window 10: home:adhoc
```

### 4.3 Window Lifecycle

**All 11 windows pre-created on startup.** Rationale:
- Cloud host rarely restarts (~2GB RAM is negligible)
- Instant switching between any mode/zone
- No startup lag on first zone entry
- Claude Code sessions warm and ready

**On `workspace-init`:**
1. Creates all 11 windows
2. Starts Claude Code in each (windows 1-10)
3. Starts dashboard TUI in window 0
4. Loads last context for each workspace

**On reboot (rare):**
- systemd runs `workspace-init` after LUKS unlock
- All windows recreated, contexts restored from saved state

### 4.4 Zone Switching

- `/work` â†’ switches to work:comms (default mode)
- `/home` â†’ switches to home:comms (default mode)
- `/work tasks` â†’ switches to work:tasks
- On zone switch: Claude auto-summarizes current context

### 4.5 Pane Structure

Freeform. Single pane default (Claude), split as needed.

### 4.6 Visual Signaling

| Element | Work Zone | Home Zone |
|---------|-----------|-----------|
| Status bar color | Blue | Green |
| Pane border | Blue | Green |
| Prompt prefix | `[work:tasks]` | `[home:comms]` |

### 4.7 Status Bar

```
Normal:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [work:tasks] â”‚ SDP-123 â”‚ 3 actionable â”‚ 14:32                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Auth problem:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [work:tasks] â”‚ SDP-123 â”‚ 3 actionable â”‚ 14:32 â”‚ âš  sdp auth          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.8 Status Dashboard (Window 0)

TUI showing:
- All workspaces with actionable counts
- Current task per workspace
- Auth status for all services
- Sync status (pending, failed)
- Calendar (next 4 hours, merged)

Auto-refresh: 30 seconds
Implementation: Python + rich/textual

### 4.9 Keyboard Navigation

Multiple navigation methods available:

**Window Numbers (built-in):**

| Keys | Destination |
|------|-------------|
| `Ctrl-b 0` | status |
| `Ctrl-b 1` | work:comms |
| `Ctrl-b 2` | work:tasks |
| `Ctrl-b 3` | work:projects |
| `Ctrl-b 4` | work:research |
| `Ctrl-b 5` | work:adhoc |
| `Ctrl-b 6` | home:comms |
| `Ctrl-b 7` | home:tasks |
| `Ctrl-b 8` | home:projects |
| `Ctrl-b 9` | home:research |
| `Alt-0` | home:adhoc (window 10) |

**Named Shortcuts (custom bindings):**

```tmux
# Zone jumps
bind-key W select-window -t 1    # Work (comms)
bind-key H select-window -t 6    # Home (comms)
bind-key S select-window -t 0    # Status

# Mode jumps (within current zone)
bind-key C run-shell "tmux select-window -t $(tmux display -p '#{?#{==:#{window_index},#{e|<:6,#{window_index}}},1,6}')"   # comms
bind-key T run-shell "tmux select-window -t $(tmux display -p '#{?#{==:#{window_index},#{e|<:6,#{window_index}}},2,7}')"   # tasks
```

**Fuzzy Finder (fzf integration):**

```tmux
bind-key f display-popup -E "tmux list-windows -F '#I: #W' | fzf --reverse | cut -d: -f1 | xargs tmux select-window -t"
```

| Keys | Action |
|------|--------|
| `Ctrl-b f` | Open fuzzy window picker |
| Type `work` | Filters to work windows |
| Type `tasks` | Filters to tasks mode |
| Enter | Switch to selected window |

**Slash Commands (from within Claude):**

```bash
/work           # Switch to work:comms
/work tasks     # Switch to work:tasks
/home           # Switch to home:comms
/status         # Switch to window 0
```

**Recommended usage:**
- Quick jumps: `Ctrl-b 0-9` for memorized positions
- Zone switch: `Ctrl-b W` or `Ctrl-b H`
- Discovery: `Ctrl-b f` when unsure
- From Claude: `/work tasks` for contextual switch with auto-save

---

## 5. Datahub

### 5.1 Purpose

Bidirectional sync between local system and external services. Captures all inputs, surfaces actionable items.

### 5.2 Storage Structure

```
~/work/datahub/
â”œâ”€â”€ items/                    â† Source of truth (flat files)
â”‚   â”œâ”€â”€ sdp-123.md
â”‚   â”œâ”€â”€ email-xyz.md
â”‚   â””â”€â”€ slack-abc.md
â”œâ”€â”€ index.sqlite              â† Derived (regeneratable)
â”œâ”€â”€ queue/
â”‚   â”œâ”€â”€ pending/
â”‚   â”œâ”€â”€ processing/
â”‚   â”œâ”€â”€ completed/
â”‚   â””â”€â”€ failed/
â”œâ”€â”€ trash/                    â† Soft delete (365-day purge)
â””â”€â”€ state/
    â””â”€â”€ sync-state.json

~/home/datahub/
â””â”€â”€ (same structure)

~/calendar/
â””â”€â”€ merged.sqlite             â† Read-only, combined view
```

### 5.3 Item Format

**ID Structure (globally unique):**

IDs follow the pattern `{source}-{external_id}` to guarantee uniqueness across all sources:

| Source | External ID | Datahub ID | File Name |
|--------|-------------|------------|-----------|
| SDP request | `123` | `sdp-123` | `sdp-123.md` |
| SDP general task | `456` | `sdp-task-456` | `sdp-task-456.md` |
| MS365 email | `AAMk...` | `ms365-AAMkAG...` | `ms365-AAMkAG....md` |
| Gmail | `18d5f...` | `gmail-18d5f...` | `gmail-18d5f....md` |
| Slack message | `1706...` | `slack-1706...` | `slack-1706....md` |
| DevOps work item | `789` | `devops-789` | `devops-789.md` |
| Adhoc (local) | timestamp | `adhoc-2026-01-29-14-32` | `adhoc-2026-01-29-14-32.md` |

**Why this matters:**
- `email-123` from Gmail and `123` from SDP are different items
- File names match IDs for easy correlation
- External IDs preserved for sync back to source

**Example item file (`sdp-123.md`):**

```markdown
---
id: sdp-123
source: sdp
type: request
title: Fix auth module
status: in-progress
zone: work
triage: act
priority: P2
tags: [auth, urgent, backend]
created: 2026-01-25
updated: 2026-01-28
---

## Description
User reports login failures...

## Notes
### 2026-01-28 14:30 (local)
Found the root cause - token refresh logic

### 2026-01-27 10:00 (sdp)
Assigned to @seth
```

**Field reference:**

| Field | Type | Values |
|-------|------|--------|
| `id` | string | `{source}-{external_id}` (globally unique) |
| `source` | string | `sdp`, `ms365`, `gmail`, `slack`, `devops`, `adhoc`, etc. |
| `type` | string | `request`, `task`, `email`, `message`, `work-item`, etc. |
| `zone` | string | `work`, `home` |
| `triage` | string | `act`, `keep`, `delete` |
| `priority` | string | `P0`, `P1`, `P2`, `P3` (optional) |
| `tags` | array | Tag names |

### 5.4 Index Schema

**Critical: SQLite is a derived index, NOT the source of truth.**

| Layer | Role | Concurrency |
|-------|------|-------------|
| Flat files (`items/*.md`) | Source of truth | `flock` per file |
| SQLite (`index.sqlite`) | Fast queries, derived | Single-writer (batch triage) |

**Why this matters:**

| Concern | Mitigation |
|---------|------------|
| SQLite locking | Only `f/triage/batch.ts` writes to SQLite (single process) |
| Data loss | Flat files survive SQLite corruption; rebuild with `dh reindex` |
| Concurrent reads | SQLite handles concurrent reads fine (WAL mode) |
| "Database is Locked" | Doesn't apply - we don't have multiple writers |

**Rebuild from source of truth:**

```bash
# If index.sqlite corrupts, regenerate from flat files
dh reindex --zone work
dh reindex --zone home
```

The index exists for fast filtering (`dh ls --triage act --zone work`). The flat files are the canonical data that gets backed up, synced, and versioned.

```sql
CREATE TABLE items (
  id TEXT PRIMARY KEY,           -- '{source}-{external_id}' format
  source TEXT NOT NULL,          -- 'sdp', 'ms365', 'gmail', etc.
  type TEXT,                     -- 'request', 'email', 'message', etc.
  zone TEXT NOT NULL,            -- 'work', 'home'
  triage TEXT DEFAULT 'keep',    -- 'act', 'keep', 'delete'
  status TEXT,
  priority TEXT,
  timestamp TEXT,
  title TEXT,
  file_path TEXT,
  updated_at TEXT
);

CREATE TABLE tags (
  id INTEGER PRIMARY KEY,
  name TEXT UNIQUE NOT NULL,     -- lowercase, normalized
  color TEXT,                    -- hex color for UI
  auto_rule TEXT                 -- optional: regex or rule for auto-tagging
);

CREATE TABLE item_tags (
  item_id TEXT REFERENCES items(id) ON DELETE CASCADE,
  tag_id INTEGER REFERENCES tags(id) ON DELETE CASCADE,
  source TEXT,          -- 'manual', 'auto', 'external' (synced from MS365/Gmail)
  created_at TEXT,
  PRIMARY KEY (item_id, tag_id)
);

CREATE INDEX idx_triage ON items(zone, triage);
CREATE INDEX idx_source ON items(source);
CREATE INDEX idx_timestamp ON items(timestamp);
CREATE INDEX idx_item_tags ON item_tags(tag_id);
```

### 5.5 Sources

| Zone | Source | System | Method | Interval |
|------|--------|--------|--------|----------|
| work | Help desk | ServiceDesk Plus Cloud | REST API | 5 min |
| work | Email | MS365 | Graph API (delta) | 5 min |
| work | Calendar | MS365 | Graph API (delta) | 5 min |
| work | Chat | Slack | slackdump | 60s |
| work | Tasks/DevOps | Azure DevOps | REST API | 5 min |
| home | Email | Gmail | history.list | 5 min |
| home | Calendar | Google Calendar | syncToken | 5 min |
| home | Chat | Telegram | getUpdates | 60s |

### 5.6 Email Sync Parameters

**Initial Sync (first run):**

| Parameter | Value | Reason |
|-----------|-------|--------|
| Lookback | 365 days | Full year of context |
| Folders | Inbox + Sent | Sent needed for conversation context |
| Batch size | 100 messages/request | API pagination |

**Ongoing Sync:**

| Parameter | Value | Method |
|-----------|-------|--------|
| MS365 | Delta query | `/messages/delta` tracks changes |
| Gmail | history.list | historyId tracks changes |
| Interval | 5 min | Balance freshness vs API limits |

**What syncs:**

| Field | Synced | Notes |
|-------|--------|-------|
| Subject, From, To, CC | Yes | Core metadata |
| Body (text) | Yes | Needed for triage/search |
| Body (HTML) | No | Text extracted only |
| Attachments | Metadata only | Download on demand |
| Categories/Labels | Yes | Mapped to tags |
| Read status | Yes | Bidirectional sync |
| Flag/Star | Yes | Bidirectional sync |

**Retention:**

| Classification | Retention |
|----------------|-----------|
| `act` | Forever |
| `keep` | Forever |
| `delete` | 365 days in trash, then purge |

**Tag sync (bidirectional):**

| Direction | Behavior |
|-----------|----------|
| Inbound | MS365 Categories / Gmail Labels â†’ local tags |
| Outbound | Local tag changes â†’ sync to MS365/Gmail |
| New tags | Created in both systems |

### 5.7 Triage

**Principle:** Accuracy and reliability over token efficiency. Always triage with full context.

**Classification (ternary):**

| Value | Meaning | Action |
|-------|---------|--------|
| `act` | Needs user action | Surfaces in workspace |
| `keep` | Reference/archive | Stored, searchable |
| `delete` | Noise/irrelevant | Moved to trash |

#### Thread-Based Triage

**Unit of triage is the thread/conversation, not individual messages.**

Rationale: A message's meaning depends on conversation context. Can't accurately triage "Re: Q3 Budget" without seeing the original and prior replies.

**Thread definition per source:**

| Source | Thread = |
|--------|----------|
| Email | All messages with same `In-Reply-To` / `References` header |
| Slack | Parent message + all replies |
| SDP | Ticket + all notes/comments |
| DevOps | Work item + all comments |
| Telegram | Messages from same sender within 5-min window |
| Calendar | Single event (no threading) |

**Triage flow:**

```
New message arrives
    â†“
Identify thread_id (existing or new)
    â†“
Gather ALL messages in thread
    â†“
Send full thread to Claude for triage
    â†“
Store classification at thread level
    â†“
All messages inherit thread classification
```

**On thread update (new reply):**

```
New reply arrives in existing thread
    â†“
Re-triage ENTIRE thread (not just new message)
    â†“
Classification may change (e.g., keep â†’ act)
```

No shortcuts. Always full context. Accuracy > tokens.

#### Context Window Management

For long threads (50+ messages), full-context triage becomes cost-prohibitive and may hit token limits.

**Sliding Window Approach:**

```
Thread has 100 messages
    â†“
Generate thread summary (cached, updated on new messages)
    â†“
Send to Claude:
  - Thread summary (who, what, key decisions)
  - Last 10 messages (recent context)
  - Metadata (participants, age, last activity)
    â†“
Triage with 90% less tokens, equivalent accuracy
```

**Thresholds:**

| Thread Size | Strategy |
|-------------|----------|
| < 20 messages | Full thread (no summarization) |
| 20-50 messages | Summary + last 10 messages |
| > 50 messages | Summary + last 10 + key messages (mentions, decisions) |

**Summary caching:** Thread summaries stored in `.threads/thread-{id}.summary.md`, regenerated only when new messages arrive.

#### Polling-Based Triage

**Principle:** Simplicity over milliseconds. Polling is easy to implement, debug, and monitor.

**Polling intervals by source urgency:**

| Source | Interval | Max Latency | Rationale |
|--------|----------|-------------|-----------|
| Slack | 60 sec | 0-60 sec | Real-time chat needs fast response |
| Telegram | 60 sec | 0-60 sec | Direct messages are time-sensitive |
| Email (MS365/Gmail) | 5 min | 0-5 min | Email is inherently async |
| SDP | 5 min | 0-5 min | Tickets aren't instant |
| DevOps | 5 min | 0-5 min | Work items aren't instant |
| Calendar | 15 min | 0-15 min | Events don't change often |

**Triage flow:**

```
Poller runs (per schedule)
    â†“
f/{source}/sync.ts fetches new/changed items
    â†“
dh write â†’ creates/updates flat files
    â†“
f/triage/batch.ts runs (every 5 min)
    â†“
Queries untriaged items
    â†“
Claude classifies threads (with sliding window for long ones)
    â†“
Updates index.sqlite
    â†“
Status TUI refreshes on next poll
```

**Why polling-only:**

| Concern | Why It's Fine |
|---------|---------------|
| Latency | 60 sec for chat is acceptable; you're not a trading desk |
| Missed messages | Polling catches everything; webhooks can fail silently |
| Complexity | No webhook endpoints, auth, deduplication |
| Debugging | Cron logs are trivial to inspect |

**Windmill makes this trivial:**

```typescript
// f/slack/sync.ts - entire poller
export async function main() {
  const since = await getLastSyncTime('slack');
  const messages = await slackClient.fetchSince(since);

  for (const msg of messages) {
    await dh.write({ source: 'slack', ...msg });
  }

  await setLastSyncTime('slack', new Date());
}
// Schedule in Windmill: * * * * * (every minute)
```

#### Storage Model

**Individual messages remain flat files (PAI principle):**

```yaml
# email-abc123.md
---
id: email-abc123
thread_id: thread-xyz789      # Links to thread
source: ms365
type: email
subject: "Re: Q3 Budget"
from: boss@work.com
date: 2026-01-30T09:00:00Z
classification: act           # Inherited from thread
---

Message body here...
```

**Thread metadata stored separately:**

```yaml
# .threads/thread-xyz789.yaml
thread_id: thread-xyz789
source: ms365
messages:
  - email-abc121
  - email-abc122
  - email-abc123
last_triage: 2026-01-30T09:15:00Z
classification: act
confidence: 0.92
reason: "Boss requesting deliverable with EOD deadline"
```

**Benefits:**
- Full context = better decisions
- Auditable: `reason` field explains classification
- Simple rule: always triage full thread
- Messages remain individual files (greppable, portable)

**Override:** `/item mark <id> <classification>` â€” overrides at thread level

**Engine:** `f/triage/batch.ts` via Windmill (Curu skill pattern)

**Schedule:** Every 15 min, processes all threads with new messages since last run

### 5.8 Sync - Inbound

Each Windmill poller script (`f/{service}/sync.ts`):
1. Gets credentials from Windmill variables/resources
2. Fetches delta/changes since last sync
3. Writes/updates flat files
4. Updates sync-state.json
5. Triggers triage + index rebuild

Windmill handles retries, logging, and error notification.

### 5.9 Sync - Outbound

Windmill on-demand scripts (`f/{service}/update-*.ts`):
1. Triggered by Curu skill or CLI command
2. Reads local item state
3. Checks for conflicts (timestamp compare)
4. Sends to external API
5. Updates local item with result

### 5.10 Slack Read/Write Split

| Operation | Method |
|-----------|--------|
| Read (poll) | slackdump (browser session) |
| Write (reply) | Slack Bot API (bot token) |

### 5.11 Conflict Scenarios

| Conflict Type | Detection | Resolution |
|---------------|-----------|------------|
| Write (additive) | Timestamp compare | Warn, allow merge |
| Write (destructive) | Timestamp compare | Warn, require confirm |
| Status change | Pre-fetch check | Block, require refresh |
| Deleted externally | 404 response | Prompt: keep/delete/recreate |
| Duplicate creation | Similarity check | Prompt: link/create/edit |
| Stale queue | Timestamp check | Review each item |
| Field-level | Per-field diff | Auto-merge if disjoint |

### 5.12 Item Completion

| Command | Behavior |
|---------|----------|
| `/task done` | Local complete only |
| `/task close "notes"` | Local + external complete |

### 5.13 Archive Policy

| Item Type | Archive After |
|-----------|---------------|
| External (SDP, DevOps) | 90 days after completion |
| Local-only (email, chat) | Never (keep forever) |

### 5.14 Export to Personal VPS

```bash
datahub export --zone home --dest rsync://personal-vps/archive
```

- Format: Flat files (portable markdown)
- Incremental: rsync handles
- Schedule: Daily cron

### 5.15 Attachments

| Direction | Behavior |
|-----------|----------|
| Inbound | Metadata only (filename, size, type) |
| Download | On demand |
| Outbound | v2 |

### 5.16 Datahub CLI (`dh`)

The `dh` tool is the unified local interface for all datahub operations. Both humans (via terminal) and automation (via Windmill/PAI) use this binary to ensure logic consistency.

**Location:** `~/bin/dh` (compiled binary)
**Implementation:** Bun (fast startup, single binary)

**Core Commands:**

| Command | Usage |
|---------|-------|
| `dh ls` | List items with rich filtering (zone, source, tags, status, triage) |
| `dh read <id>` | Output item content (JSON or Markdown format) |
| `dh write` | Create/update item (handles validation, ID generation, locking) |
| `dh triage [--batch]` | Run classification (calls PAI for AI triage) |
| `dh sync [source]` | Trigger Windmill sync jobs (manual override) |
| `dh search <query>` | Full-text search across all items |
| `dh queue` | Show pending outbound operations |

**Why this matters:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Single Source of Truth                                         â”‚
â”‚                                                                 â”‚
â”‚  User (terminal)  â”€â”€â”                                           â”‚
â”‚                     â”œâ”€â”€â–º dh binary â”€â”€â–º Flat files + SQLite     â”‚
â”‚  Windmill scripts â”€â”€â”¤              â”€â”€â–º Queue management         â”‚
â”‚                     â”‚              â”€â”€â–º Validation               â”‚
â”‚  PAI skills â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”€â”€â–º Locking                  â”‚
â”‚                                                                 â”‚
â”‚  Everyone uses the same logic. No drift between manual and      â”‚
â”‚  automated operations.                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Examples:**

```bash
# List actionable items in work zone
dh ls --zone work --triage act

# Read item as JSON (for scripting)
dh read sdp-123 --format json

# Create new item (validates schema, generates ID, places in queue)
dh write --source local --zone work --title "Investigate memory leak"

# Trigger sync for specific source
dh sync sdp

# Search across all items
dh search "authentication bug"
```

**Windmill integration:**

```typescript
// f/sdp/sync.ts
import { DatahubCLI } from '../lib/dh';

export async function main() {
  const dh = new DatahubCLI();

  for (const ticket of await fetchSDPTickets()) {
    await dh.write({
      id: `sdp-${ticket.id}`,
      source: 'sdp',
      zone: 'work',
      ...ticket
    });
  }
}
```

---

## 6. Authentication

### 6.1 Simplified Auth Model

**BWS is bootstrap-only.** On startup, secrets sync to Windmill variables. Windmill handles all runtime credential management.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Bootstrap (once on startup)                                     â”‚
â”‚                                                                 â”‚
â”‚   BWS â”€â”€syncâ”€â”€â–¶ Windmill Variables                             â”‚
â”‚                                                                 â”‚
â”‚   f/ops/bws-to-windmill.ts                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Runtime (Windmill handles everything)                           â”‚
â”‚                                                                 â”‚
â”‚   OAuth tokens: Windmill resources (auto-refresh)              â”‚
â”‚   API keys: Windmill variables (static)                        â”‚
â”‚   AWS roles: On-demand STS assume (f/aws/get-session.ts) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Eliminated:** auth-keeper.sh (~1800 lines). Windmill replaces it entirely.

### 6.2 Bootstrap Chain

BWS token is the ONE credential that exists outside Windmill - it's the root of the trust chain.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Bootstrap Chain (on startup/reboot)                             â”‚
â”‚                                                                 â”‚
â”‚ 1. LUKS unlock                                                  â”‚
â”‚    - Passphrase (interactive) + keyfile (from BWS cache)       â”‚
â”‚    - Decrypts /data volume                                      â”‚
â”‚                                                                 â”‚
â”‚ 2. BWS token loaded                                             â”‚
â”‚    - /data/.bws-token persisted on encrypted volume            â”‚
â”‚    - Created during first-time setup, survives reboots         â”‚
â”‚                                                                 â”‚
â”‚ 3. systemd starts Windmill                                      â”‚
â”‚    - windmill-worker.service has EnvironmentFile=/data/.env    â”‚
â”‚    - /data/.env contains BWS_ACCESS_TOKEN                       â”‚
â”‚                                                                 â”‚
â”‚ 4. Initial sync runs                                            â”‚
â”‚    - f/ops/bws-sync.ts triggers on Windmill startup            â”‚
â”‚    - Pulls all secrets from BWS â†’ Windmill variables           â”‚
â”‚                                                                 â”‚
â”‚ 5. Windmill now self-sufficient                                 â”‚
â”‚    - All credentials available as variables/resources          â”‚
â”‚    - Scheduled sync every 30min keeps them fresh               â”‚
â”‚    - OAuth resources handle their own refresh cycles           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Bootstrap files on encrypted volume:**

| File | Purpose |
|------|---------|
| `/data/.bws-token` | BWS access token (root credential) |
| `/data/.env` | Environment file for systemd services |
| `/data/.luks-keyfile` | LUKS keyfile (cached from BWS) |

**First-time setup creates these files.** Subsequent reboots only need LUKS passphrase.

### 6.3 Secret Naming Convention

```
{zone}-{service}-{item}

Examples:
work-ms365-tenant-id
work-ms365-client-id
work-sdp-api-token
home-google-client-id
home-telegram-bot-token
```

### 6.4 Auth Types by Service

| System | Auth Type | Windmill Handles |
|--------|-----------|------------------|
| MS365 | OAuth2 (Service Principal) | âœ“ Auto-refresh via resource |
| SDP | OAuth2 (Zoho) | âœ“ Auto-refresh via resource |
| DevOps | PAT | Static variable (warn on expiry) |
| Slack | OAuth2 | âœ“ Auto-refresh via resource |
| Google | OAuth2 | âœ“ Auto-refresh via resource |
| Telegram | Bot token | Static variable (never expires) |
| AWS | STS AssumeRole | On-demand via script |

### 6.5 Windmill Credential Access

**In scripts:**

```typescript
// API keys (static)
const token = await wmill.getVariable("work-sdp-api-token");

// OAuth tokens (auto-refreshed)
const resource = await wmill.getResource("work-ms365");
const accessToken = resource.token;  // Always valid

// AWS (on-demand assume)
const creds = await wmill.runScript("aws/get-session", {
    account: "prod",
    role: "ReadOnly"
});
```

**No more:**
- `auth-keeper get work-ms365`
- `auth-keeper refresh work-sdp`
- `auth-keeper status`

Windmill UI shows all credential status.

### 6.6 Smart Discovery

Windmill scheduled script detects new BWS entries:

```
f/ops/bws-sync.ts (runs every 30 min)
    â†“
"New secret found: jira-api-token"
    â†“
Creates Windmill variable
    â†“
Creates local integration task in datahub
```

**Auto-created integration task:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  New service "jira" detected in BWS                             â”‚
â”‚                                                                 â”‚
â”‚  Creates datahub item:                                          â”‚
â”‚    id: local-integrate-jira-{timestamp}                         â”‚
â”‚    source: local                                                â”‚
â”‚    type: integration-task                                       â”‚
â”‚    title: "Build integration for jira"                          â”‚
â”‚                                                                 â”‚
â”‚  With checklist:                                                â”‚
â”‚    â–¡ Create Windmill scripts (f/jira/*.ts)                â”‚
â”‚    â–¡ Add schedule for sync                                      â”‚
â”‚    â–¡ Test via Windmill UI                                       â”‚
â”‚    â–¡ Document in CLAUDE.md                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Simplified checklist** (no auth-keeper, no separate Curu skill (PAI)):

```markdown
## Checklist for new integration: jira

- [ ] **Scripts**: Create in Windmill
  - `f/jira/sync.ts` - scheduled poller
  - `f/jira/get-issue.ts` - on-demand query
  - `f/jira/update-issue.ts` - write-back

- [ ] **Schedule**: Add in Windmill UI
  - `jira/sync.ts` â†’ `*/5 * * * *`

- [ ] **Test**: Run manually in Windmill UI

- [ ] **Document**: Update CLAUDE.md
```

One place to add (Windmill). One skill routes all (Windmill Curu skill (PAI)).

### 6.7 Credential Setup Wizard

Interactive wizard for first-time setup and adding new services.

**Run:** `imladris-setup` or `f/ops/credential-wizard.ts`

**Flow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IMLADRIS CREDENTIAL WIZARD                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Step 1: BWS Access                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚  Enter BWS access token: ****************************           â”‚
â”‚  âœ“ Connected to BWS                                             â”‚
â”‚                                                                 â”‚
â”‚  Step 2: Service Registry                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚  Reading known services from BWS...                             â”‚
â”‚                                                                 â”‚
â”‚  Found in BWS:                 Status in Windmill:              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚
â”‚  work-sdp-api-token            âœ“ Synced                         â”‚
â”‚  work-ms365-client-id          âœ“ Synced                         â”‚
â”‚  work-ms365-client-secret      âœ“ Synced                         â”‚
â”‚  work-slack-token              âœ“ Synced                         â”‚
â”‚  home-telegram-bot-token       âœ“ Synced                         â”‚
â”‚  home-google-client-id         âš  Missing in Windmill            â”‚
â”‚                                                                 â”‚
â”‚  Step 3: Sync Missing                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚
â”‚  Sync home-google-client-id to Windmill? [Y/n]: y               â”‚
â”‚  âœ“ Synced                                                       â”‚
â”‚                                                                 â”‚
â”‚  Step 4: Add New Services                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚  Add a new service? [y/N]: y                                    â”‚
â”‚                                                                 â”‚
â”‚  Available templates:                                           â”‚
â”‚  1. API Key service (Ramp, Securonix, etc.)                    â”‚
â”‚  2. OAuth2 service (MS365, Google, Slack)                      â”‚
â”‚  3. AWS cross-account                                           â”‚
â”‚  4. Custom                                                      â”‚
â”‚                                                                 â”‚
â”‚  Select [1-4]: 1                                                â”‚
â”‚                                                                 â”‚
â”‚  Service name: ramp                                             â”‚
â”‚  Zone [work/home]: work                                         â”‚
â”‚  API key: ****************************                          â”‚
â”‚                                                                 â”‚
â”‚  Creating:                                                      â”‚
â”‚    BWS: work-ramp-api-key                                       â”‚
â”‚    Windmill variable: work-ramp-api-key                         â”‚
â”‚    Integration task: local-integrate-ramp-{timestamp}           â”‚
â”‚                                                                 â”‚
â”‚  âœ“ Done. Run /task list to see integration checklist.           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What it does:**

| Step | Action |
|------|--------|
| 1. BWS Access | Validates BWS token, caches to `/data/.bws-token` |
| 2. Service Registry | Reads all `{zone}-{service}-*` secrets from BWS |
| 3. Sync Missing | Compares BWS to Windmill, syncs any gaps |
| 4. Add New | Prompts for new services, creates in both BWS and Windmill |

**OAuth services (MS365, Google, Slack):**

For OAuth, wizard provides links and instructions:

```
Service: ms365
Type: OAuth2 (Service Principal)

Required steps:
1. Go to: https://portal.azure.com â†’ App Registrations
2. Create new registration: "imladris-ms365"
3. Add API permissions: Mail.Read, Mail.Send, Calendar.Read
4. Create client secret
5. Enter values below:

   Tenant ID: ___________________________
   Client ID: ___________________________
   Client Secret: ___________________________

Creating Windmill OAuth2 resource...
âœ“ work-ms365 resource created

Test connection? [Y/n]: y
âœ“ Successfully connected to MS365 Graph API
```

**Run on first setup and whenever adding new services.**

### 6.8 Cloud Account Registry

BWS tracks accessible cloud accounts for discoverability:

**AWS Accounts (BWS: `aws-accounts`):**

```json
[
  {
    "id": "111111111111",
    "name": "prod",
    "alias": "org-prod",
    "roles": ["ReadOnlyAccess", "AdminAccess"],
    "purpose": "Production environment",
    "default_role": "ReadOnlyAccess"
  },
  {
    "id": "222222222222",
    "name": "dev",
    "alias": "org-dev",
    "roles": ["ReadOnlyAccess", "AdminAccess"],
    "purpose": "Development/staging",
    "default_role": "AdminAccess"
  }
]
```

**GCP Projects (BWS: `gcp-projects`):**

```json
[
  {
    "id": "my-project-123",
    "name": "prod",
    "purpose": "Production GCP"
  }
]
```

**AWS Access Model:**
- Instance profile on host provides base credentials
- Profile can assume roles in all registered accounts
- No credential management needed - automatic via EC2 metadata
- Windmill script `f/ops/aws-config-gen.ts` generates `~/.aws/config` from BWS

**Generated AWS Config:**

```ini
[profile org-prod-readonly]
role_arn = arn:aws:iam::111111111111:role/ReadOnlyAccess
credential_source = Ec2InstanceMetadata

[profile org-prod-admin]
role_arn = arn:aws:iam::111111111111:role/AdminAccess
credential_source = Ec2InstanceMetadata

[profile org-dev-readonly]
role_arn = arn:aws:iam::222222222222:role/ReadOnlyAccess
credential_source = Ec2InstanceMetadata
```

### 6.9 Offline Limitation

Claude via Bedrock requires network. Offline mode is view-only (grep datahub).

---

## 7. Commands

### 7.1 Workspace Commands

```bash
/work [mode]           # Switch to work zone (default: comms)
/home [mode]           # Switch to home zone (default: comms)
/status                # Window 0 - full dashboard
```

### 7.2 Task Commands

```bash
/task list             # Show actionable from datahub
/task show             # Current task details
/task start <id>       # Explicit start
/task switch <id>      # Pause current, start new
/task note "text"      # Add note (syncs)
/task log <time> "text"# Log time (syncs)
/task status <status>  # Change status (syncs)
/task pause            # Save context, no completion
/task done             # Local complete only
/task close "notes"    # Local + external complete
/task create <source> "title"  # Create new item
```

### 7.3 Item Commands

```bash
/inbox                 # Show actionable in current workspace
/item show <id>        # View item details
/item mark <id> <class># Override triage (act/keep/delete)
/item done <id>        # Mark item complete
```

### 7.4 Search Commands

```bash
/search "query"              # Search all
/search "query" --zone work  # Filter by zone
/search "query" --source sdp # Filter by source
/search "query" --tag urgent # Filter by tag
/search --tag finance        # All items with tag (no text query)
```

### 7.5 Calendar Commands

```bash
/calendar              # Today's merged view
/calendar week         # Week view
/calendar add --work "title"  # Create work event
/calendar add --home "title"  # Create home event
```

### 7.6 Communication Commands

```bash
/slack                 # Show actionable Slack items
/slack reply <thread>  # Reply to thread
```

### 7.7 Attachment Commands

```bash
/attachment list <item-id>     # List attachments
/attachment download <att-id>  # Download attachment
```

### 7.8 Cloud Commands

Cloud access via Windmill scripts and generated AWS config:

```bash
# AWS (via Curu skill â†’ Windmill)
/aws list                               # List all accessible accounts
/aws session <account>                  # Get temp credentials for interactive use
/aws session <account> --role Admin     # Specific role
/aws whoami                             # Current account/role context

# Direct AWS CLI usage (uses generated profiles)
aws --profile org-prod-readonly s3 ls
aws --profile org-dev-admin ec2 describe-instances

# GCP (via Curu skill â†’ Windmill)
/gcp list                               # List all accessible projects
/gcp session <project>                  # Set up project context
```

### 7.9 System Commands

```bash
/ops status            # Windmill job overview
/ops failures          # Show failed jobs
/ops run <script>      # Trigger Windmill script manually
/sync status           # Sync status across all sources
```

### 7.10 Tag Commands

```bash
# View and manage tags
/tag list                      # List all tags
/tag list --item <id>          # Show tags for item
/tag search <tag>              # Find items with tag

# Apply tags
/tag add <item-id> <tag>       # Add tag to item (syncs to external)
/tag remove <item-id> <tag>    # Remove tag from item
/tag bulk <tag> <query>        # Add tag to all items matching query

# Manage tag definitions
/tag create <name> [--color hex] [--rule regex]  # Create new tag
/tag delete <name>             # Delete tag (removes from all items)
/tag auto                      # Run auto-tagging rules on untagged items

# External sync
/tag sync                      # Force sync tags with MS365/Gmail
```

**Auto-tagging rules (optional):**

| Tag | Rule Example |
|-----|--------------|
| `urgent` | Subject contains "urgent" or "asap" |
| `finance` | From contains "@finance" |
| `meeting` | Has calendar attachment |
| `newsletter` | From matches known newsletter domains |

---

## 8. Infrastructure

### 8.1 Summary (from Imladris 1.0)

| Component | Config |
|-----------|--------|
| Instance | m7g.xlarge (ARM64/Graviton), Ubuntu 24.04 |
| Root volume | 50GB gp3 (6000 IOPS, 500 MiB/s) |
| Data volume | 100GB gp3, LUKS encrypted ("hall-of-fire") |
| Snapshots | Hourly via DLM, 24 retained |
| Offsite backup | Secondary (TBD) |
| Network | VPC, no public ingress, Tailscale only |
| IAM | EBS attach, Bedrock, cross-account assume |

### 8.2 Terraform Structure

Copy from existing imladris repo:
- `main.tf` - VPC, EC2, EBS, IAM, DLM
- `variables.tf` - Configuration
- `outputs.tf` - Instance info
- `versions.tf` - Provider versions

### 8.3 Nix + home-manager

Declarative configuration for:
- Shell (zsh)
- Packages (see table below)
- Claude Code
- Curu skills (PAI)
- Dotfiles

#### Installed Tools

| Category | Tools |
|----------|-------|
| Core | git, zsh, tmux, vim, curl, jq, ripgrep, fd, fzf |
| Languages | nodejs_20, bun, python3 |
| Cloud | aws-cli, gcloud, kubectl, terraform |
| Containers | docker, docker-compose |
| GitHub | gh (GitHub CLI) |
| Secrets | bws (Bitwarden Secrets CLI) |
| Network | tailscale |
| AI | claude (via npm, see below) |

#### Claude Code Packaging (Known Friction)

Claude Code is distributed via npm (`@anthropic-ai/claude-code`). NixOS's immutable store creates friction with npm global installs.

**The problem:**

| Approach | Issue |
|----------|-------|
| `npm install -g` | Fails on NixOS (can't write to immutable paths) |
| Nix derivation | Maintenance overhead (rehash on every update) |
| `~/.npm-global` | Works, but breaks declarative promise |

**Recommended approach: Pragmatic imperative install**

```nix
# In home.nix - set up the environment
home.sessionVariables = {
  NPM_CONFIG_PREFIX = "$HOME/.npm-global";
};

home.sessionPath = [ "$HOME/.npm-global/bin" ];

# Ensure Node.js version is pinned
home.packages = with pkgs; [
  nodejs_20  # Pin to specific version
];
```

```bash
# One-time setup (imperative, but contained)
npm config set prefix ~/.npm-global
npm install -g @anthropic-ai/claude-code
```

**Trade-offs accepted:**

| Concern | Mitigation |
|---------|------------|
| Not declarative | Contained to ~/.npm-global, documented in CLAUDE.md |
| Rollback doesn't work | Claude Code updates rarely break; can pin version |
| Node version mismatch | Pin nodejs_20 in Nix, update both together |
| Rebuilds lose it | Add to imladris-init.sh bootstrap |

**Why not full Nix packaging:**

| Factor | Reason to skip |
|--------|----------------|
| Update frequency | Claude Code updates weekly; constant rehashing |
| Native bindings | Many npm deps use node-gyp; patchelf is fragile |
| Maintenance cost | Time better spent on features than packaging |
| Risk | Claude Code is well-tested; packaging bugs are self-inflicted |

**Bootstrap recovery:**

```bash
# In imladris-init.sh
if ! command -v claude &> /dev/null; then
    echo "Installing Claude Code..."
    npm install -g @anthropic-ai/claude-code
fi
```

This ensures Claude Code is restored after system rebuild.

### 8.4 Storage Layout

All stateful data lives on the LUKS-encrypted data volume (`/data`). Root volume is ephemeral/rebuildable.

**LUKS Volume Structure:**

```
/data/                          â† LUKS mounted (hall-of-fire)
â”œâ”€â”€ work/
â”‚   â”œâ”€â”€ datahub/
â”‚   â”‚   â”œâ”€â”€ items/
â”‚   â”‚   â”œâ”€â”€ index.sqlite
â”‚   â”‚   â”œâ”€â”€ queue/
â”‚   â”‚   â”œâ”€â”€ trash/
â”‚   â”‚   â””â”€â”€ state/
â”‚   â””â”€â”€ tasks/                  â† task context files
â”‚
â”œâ”€â”€ home/
â”‚   â”œâ”€â”€ datahub/
â”‚   â”‚   â””â”€â”€ (same structure)
â”‚   â””â”€â”€ tasks/
â”‚
â”œâ”€â”€ repos/                      â† zone-based git repos (ghq)
â”‚   â”œâ”€â”€ work/
â”‚   â”‚   â””â”€â”€ github.com/
â”‚   â”‚       â””â”€â”€ work-org/...
â”‚   â””â”€â”€ home/
â”‚       â””â”€â”€ github.com/
â”‚           â””â”€â”€ sethdf/...
â”‚
â”œâ”€â”€ calendar/
â”‚   â””â”€â”€ merged.sqlite
â”‚
â”œâ”€â”€ claude/                     â† ~/.claude symlinked here
â”‚   â”œâ”€â”€ settings.json
â”‚   â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ skills/
â”‚   â””â”€â”€ projects/
â”‚
â”œâ”€â”€ ssh/                        â† ~/.ssh symlinked here
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ bws/
â”‚   â”œâ”€â”€ slackdump/
â”‚   â”œâ”€â”€ windmill/
â”‚   â””â”€â”€ tmux/
â”‚
â””â”€â”€ backups/                    â† local backup staging
```

**Symlinks from Home Directory:**

| Symlink | Target |
|---------|--------|
| `~/.claude` | `/data/claude` |
| `~/.ssh` | `/data/ssh` |
| `~/.bws` | `/data/config/bws` |
| `~/.slackdump` | `/data/config/slackdump` |
| `~/repos` | `/data/repos` |
| `~/work` | `/data/work` |
| `~/home` | `/data/home` |
| `~/calendar` | `/data/calendar` |

**Stateful Items Checklist:**

| Item | Location | On LUKS |
|------|----------|---------|
| Datahub (work/home) | `/data/work/datahub/`, `/data/home/datahub/` | âœ“ |
| Calendar merged | `/data/calendar/` | âœ“ |
| Task context | `/data/work/tasks/`, `/data/home/tasks/` | âœ“ |
| Claude sessions | `/data/claude/` | âœ“ |
| PAI memory/skills | `/data/claude/memory/`, `/data/claude/skills/` | âœ“ |
| SSH keys | `/data/ssh/` | âœ“ |
| Git repos (work) | `/data/repos/work/` | âœ“ |
| Git repos (home) | `/data/repos/home/` | âœ“ |
| BWS token cache | `/data/config/bws/` | âœ“ |
| slackdump auth | `/data/config/slackdump/` | âœ“ |
| Windmill data | `/data/config/windmill/` | âœ“ |
| Sync queue | `/data/*/datahub/queue/` | âœ“ |
| tmux resurrect | `/data/config/tmux/` | âœ“ |

**Ephemeral (Root Volume):**

| Item | Reason |
|------|--------|
| Nix store | Rebuildable from flake |
| Packages | Rebuildable |
| OS | Rebuildable |
| Temp files | Ephemeral by nature |

### 8.5 Update Service

Automatic updates with rollback on failure.

**Components Updated:**

| Component | Source | Frequency |
|-----------|--------|-----------|
| Claude Code | npm (@anthropic-ai/claude-code) | Daily |
| PAI | GitHub (danielmiessler/PAI) | Daily |
| Spec Kit | GitHub (github/spec-kit) | Daily |
| MCP servers | npm (@modelcontextprotocol/*) | Daily |
| Skills repos | GitHub (curu-skills, anthropics/skills) | Daily |
| Nix packages | nixpkgs | Weekly |

**Workflow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  update-service (systemd timer, daily 03:00)                    â”‚
â”‚                                                                 â”‚
â”‚  For each component with update available:                      â”‚
â”‚    1. Snapshot current state                                    â”‚
â”‚    2. Apply update                                              â”‚
â”‚    3. Run validation tests                                      â”‚
â”‚    4. If tests fail â†’ rollback â†’ notify failure                 â”‚
â”‚    5. If tests pass â†’ notify success                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Validation Tests:**

| Component | Validation |
|-----------|------------|
| Claude Code | `claude --version`, basic prompt test |
| PAI | Skills load without error |
| Spec Kit | CLI health check |
| MCP servers | Health check endpoints |
| Nix | `nix build` succeeds |

**Notifications:** Via configured channel (SimpleX, Telegram, or status dashboard)

**Rollback:** Each component uses appropriate rollback:
- Nix: `nix profile rollback`
- npm: Previous version pinned in lockfile
- Git repos: `git checkout` to previous commit

### 8.6 Repository Structure

Zone-based repository organization using `ghq`.

**Directory Structure:**

```
/data/repos/
â”œâ”€â”€ work/
â”‚   â””â”€â”€ github.com/
â”‚       â”œâ”€â”€ work-org/
â”‚       â”‚   â”œâ”€â”€ project-alpha/
â”‚       â”‚   â””â”€â”€ project-beta/
â”‚       â””â”€â”€ azure.com/
â”‚           â””â”€â”€ work-org/
â”‚               â””â”€â”€ devops-repo/
â”‚
â””â”€â”€ home/
    â””â”€â”€ github.com/
        â”œâ”€â”€ sethdf/
        â”‚   â”œâ”€â”€ imladris/
        â”‚   â”œâ”€â”€ curu-skills/
        â”‚   â””â”€â”€ personal-projects/
        â””â”€â”€ danielmiessler/
            â””â”€â”€ PAI/
```

**ghq Configuration:**

```bash
# Workspace sets GHQ_ROOT based on zone
# In work zone:
export GHQ_ROOT=/data/repos/work
ghq get github.com/work-org/project

# In home zone:
export GHQ_ROOT=/data/repos/home
ghq get github.com/sethdf/imladris
```

**Workspace Integration:**

| Zone | GHQ_ROOT | Effect |
|------|----------|--------|
| work | `/data/repos/work` | `ghq list` shows only work repos |
| home | `/data/repos/home` | `ghq list` shows only home repos |

**Benefits:**

- Repos physically separated by zone
- Can export home zone to personal VPS independently
- `ghq list | fzf` shows contextually relevant repos
- No accidental work on wrong repo in wrong zone

---

## 9. User Scenarios

### 9.1 Daily Startup

1. `ssh imladris`
2. `tmux attach`
3. Window 0 shows status dashboard
4. `/work` to start
5. Claude shows last task context

### 9.2 Working on a Task

1. Enter `work:tasks`
2. Auto-loads current task context
3. Work with Claude
4. `/task note "Found bug"` - syncs to SDP
5. `/task switch SDP-456` - saves context, loads new

### 9.3 Processing Email

1. `/work comms`
2. `/inbox` - shows actionable emails
3. "Reply to this email..."
4. Claude drafts, user approves
5. `/item done email-xyz`

### 9.4 Zone Switch

1. Working in `work:tasks`
2. `/home`
3. Claude auto-summarizes work context
4. Enters `home:comms`
5. Claude shows home context

### 9.5 Auth Issue

1. Status bar shows `âš  sdp auth`
2. Check Windmill UI â†’ Resources for token status
3. OAuth tokens auto-refresh; if expired, re-authenticate in Windmill UI
4. For API keys, update in BWS â†’ run `f/ops/bws-sync.ts`

### 9.6 First-Time Setup (Bootstrap Guide)

Complete setup in this exact order. Each step depends on previous steps.

**Phase 1: Infrastructure (Terraform)**

```
1. Clone imladris repo locally
2. Configure terraform.tfvars (git-crypt encrypted)
3. terraform apply
4. Wait for instance to be ready
5. Note: Instance has Nix, Tailscale, base packages
```

**Phase 2: Access & Secrets**

```
6. tailscale up (from local machine, approve device)
7. ssh imladris (first connection)
8. BWS_ACCESS_TOKEN=<token> bws-init   â† Unlocks secrets access
9. imladris-unlock                      â† LUKS passphrase prompt
10. Verify: ls /data                    â† Should show directories
```

**Phase 3: Authentication Setup**

```
All auth configured in Windmill UI (http://localhost:8000).

11. Add API keys to BWS (work-sdp-api-token, etc.)
12. Run f/ops/bws-sync.ts to populate Windmill variables
13. For OAuth services, create Resources in Windmill:
    - work-ms365: OAuth2 resource (service principal)
    - work-slack: OAuth2 resource
    - home-google: OAuth2 resource
14. For static tokens, verify Windmill variables populated
15. Check Windmill UI â†’ Resources/Variables for green status
```

**Phase 4: Initial Sync**

```
18. datahub sync --initial              â† 365-day email backfill
    â”œâ”€â”€ ms365-mail: ~10-30 min (depends on volume)
    â”œâ”€â”€ gmail: ~10-30 min
    â”œâ”€â”€ sdp: ~1-5 min
    â”œâ”€â”€ devops: ~1-5 min
    â”œâ”€â”€ slack: ~5-15 min (slackdump)
    â””â”€â”€ telegram: instant
19. datahub triage --batch              â† Initial classification
20. datahub status                      â† Verify counts
```

**Phase 5: Workspace Setup**

```
21. tmux new -s main                    â† Create main session
22. Run workspace-init                  â† Creates all 11 windows
23. /work                               â† Enter first workspace
24. Claude loads, shows context
```

**Verification Checklist:**

| Check | Command | Expected |
|-------|---------|----------|
| LUKS mounted | `mount \| grep /data` | `/dev/mapper/hall-of-fire on /data` |
| BWS accessible | `bws-get test` | No error |
| All auth valid | Windmill UI â†’ Resources | All resources green |
| Datahub populated | `datahub stats` | Item counts > 0 |
| Workspaces ready | `tmux list-windows` | 11 windows |

**If Something Fails:**

| Failure | Recovery |
|---------|----------|
| LUKS won't unlock | Check passphrase, verify BWS keyfile exists |
| Auth setup fails | Check BWS secrets exist with correct names |
| Initial sync hangs | Check Windmill UI â†’ Runs for that service |
| Triage errors | Check Claude/Bedrock connectivity |

**Time Estimate:**

| Phase | Duration |
|-------|----------|
| Terraform | 5-10 min |
| Access & Secrets | 5 min |
| Auth Setup | 15-20 min (browser flows) |
| Initial Sync | 30-60 min (background OK) |
| Workspace Setup | 2 min |
| **Total** | **~1-1.5 hours** |

---

## 10. Coding Methodology

### 10.1 Spec Kit as Standard

All coding work uses [Spec Kit](https://github.com/github/spec-kit) (GitHub's specification-driven development toolkit) for consistency and reliability.

**Rationale:**
- Specs improve AI code reliability by 2.5-3x
- Consistent methodology eliminates "is this complex enough?" decisions
- Tests are defined in spec phase, not afterthought
- Single user = no team review, so specs provide the "second pair of eyes"

### 10.2 Four-Phase Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 1: SPECIFY                                               â”‚
â”‚  /specify "feature description"                                 â”‚
â”‚  â†’ Writes spec.md with requirements, constraints, acceptance    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 2: PLAN                                                  â”‚
â”‚  /plan                                                          â”‚
â”‚  â†’ Technical design, architecture decisions, file changes       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 3: BREAKDOWN                                             â”‚
â”‚  /breakdown                                                     â”‚
â”‚  â†’ Break into implementable units with test criteria            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 4: IMPLEMENT                                             â”‚
â”‚  /implement                                                     â”‚
â”‚  â†’ Code + tests, referencing spec for acceptance criteria       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 10.3 Integration with PAI

| Layer | Tool | Purpose |
|-------|------|---------|
| **What to build** | Spec Kit | Requirements, constraints, acceptance criteria |
| **How Claude behaves** | Curu Skills (PAI) | Execution patterns, response formats |
| **Project context** | CLAUDE.md | Repository-specific rules and conventions |
| **External data** | MCP | Structured connections to APIs and services |

**Workflow integration:**

```
User request
    â”‚
    â–¼
Spec Kit (/specify)     â† Define WHAT
    â”‚
    â–¼
Curu Skills (PAI)              â† Define HOW (patterns, methodology)
    â”‚
    â–¼
Claude Code execution   â† Do the work
    â”‚
    â–¼
Spec Kit (/verify)      â† Confirm acceptance criteria met
```

### 10.4 Spec Storage

Specs are stored with their associated code:

```
project/
â”œâ”€â”€ .specs/
â”‚   â”œâ”€â”€ feature-auth.md
â”‚   â”œâ”€â”€ feature-auth.plan.md
â”‚   â”œâ”€â”€ feature-auth.tasks.md
â”‚   â””â”€â”€ archive/           â† Completed specs
â”œâ”€â”€ src/
â””â”€â”€ tests/
```

For datahub items (SDP tickets, etc.), specs link to the item:

```markdown
---
id: feature-auth
linked_item: sdp-456
status: implementing
---

## Specification
...
```

### 10.5 When Specs Are Required

**Always.** No exceptions for "simple" changes.

| Change Type | Spec Depth |
|-------------|------------|
| Bug fix | Minimal (problem, root cause, fix, test) |
| New feature | Full (requirements, design, tasks, tests) |
| Refactor | Medium (goal, approach, validation) |
| Config change | Minimal (what, why, rollback) |

### 10.6 Spec Kit Commands

```bash
# Workspace-aware (uses current zone/mode context)
/specify "description"     # Start spec for new work
/plan                      # Generate technical plan from spec
/breakdown                 # Break plan into implementable units
/implement                 # Execute current unit
/verify                    # Check acceptance criteria
/spec status               # Show current spec state
/spec list                 # List active specs in workspace
```

**Command naming rationale:**

| Command | Purpose | Why this name |
|---------|---------|---------------|
| `/task` | Datahub task management | Primary workflow, used constantly |
| `/breakdown` | Spec Kit phase 3 | Avoids collision, describes action |

### 10.7 Git Automation

Git commits and pushes happen automatically. Never think about "did I commit?" or "did I push?"

**Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  While working (continuous)                                     â”‚
â”‚                                                                 â”‚
â”‚  File change detected (debounce 30s)                            â”‚
â”‚      â†“                                                          â”‚
â”‚  Auto-commit to wip/{spec-id}                                   â”‚
â”‚      â†“                                                          â”‚
â”‚  Auto-push to GitHub                                            â”‚
â”‚                                                                 â”‚
â”‚  Invisible. You never think about this.                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  On /verify pass                                                â”‚
â”‚                                                                 â”‚
â”‚  Squash WIP commits â†’ single clean commit                       â”‚
â”‚      â†“                                                          â”‚
â”‚  Merge to main                                                  â”‚
â”‚      â†“                                                          â”‚
â”‚  Push main to GitHub                                            â”‚
â”‚      â†“                                                          â”‚
â”‚  Delete WIP branch (local + remote)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What this guarantees:**

| Concern | Handled |
|---------|---------|
| Work committed | Always (every 30s after change) |
| Work pushed to GitHub | Always (after every commit) |
| Work safe if instance dies | Yes, on GitHub |
| Git history clean | Yes, squashed on `/verify` |
| Manual git commands needed | Never (unless you want to) |

**Implementation:**

- `gitwatch` daemon per active repo
- Hooks into Spec Kit `/verify` for squash-merge
- WIP branch naming: `wip/{spec-id}` (e.g., `wip/feature-auth`)
- Commit messages: auto-generated with timestamp + changed files

**Branch flow:**

```
main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                      â†‘
                                      â”‚ squash merge
                                      â”‚
wip/feature-auth â”€â”€â—â”€â”€â—â”€â”€â—â”€â”€â—â”€â”€â—â”€â”€â—â”€â”€â”˜
                   â†‘  â†‘  â†‘  â†‘  â†‘  â†‘
                   auto-commits (invisible)
```

**Security: Preventing Secret Leaks**

Auto-push to GitHub requires safeguards against accidental secret exposure.

**Pre-commit scanning (mandatory):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Before every auto-commit:                                       â”‚
â”‚                                                                 â”‚
â”‚  1. Run secrets scanner (gitleaks/trufflehog)                   â”‚
â”‚  2. If secrets detected â†’ BLOCK commit â†’ alert user             â”‚
â”‚  3. If clean â†’ proceed with commit + push                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Mandatory .gitignore patterns:**

```gitignore
# Secrets - NEVER commit
.env
.env.*
*.pem
*.key
*credentials*
*secret*
auth.json
tokens.json

# Imladris-specific
/datahub/          # Synced data contains PII
/.claude/          # May contain sensitive context
/queue/            # Pending writes may have tokens
```

**What triggers a block:**

| Pattern | Example | Action |
|---------|---------|--------|
| API keys | `AKIA...`, `sk-...` | Block + alert |
| Private keys | `-----BEGIN RSA PRIVATE KEY-----` | Block + alert |
| Passwords in code | `password = "..."` | Block + alert |
| AWS credentials | `aws_secret_access_key` | Block + alert |
| High entropy strings | Random 40+ char strings | Warn (may be false positive) |

**Recovery when blocked:**

```bash
# gitwatch alerts: "Commit blocked: potential secret in config.js:42"

# Option 1: Remove the secret
vim config.js                    # Remove secret, use env var instead

# Option 2: False positive - allowlist
echo "config.js:42" >> .gitleaks-allowlist

# gitwatch resumes automatically after fix
```

**WIP branches are semi-public:**

| Concern | Mitigation |
|---------|------------|
| WIP pushed to GitHub | Use private repos for sensitive work |
| Branch visible to collaborators | Secrets scanner prevents exposure |
| Squash loses history | WIP history preserved locally in reflog |

### 10.7 Development Guidelines

When adding new features or commands, follow this checklist to maintain codebase hygiene.

#### Feature Checklist

Every new feature must complete these steps:

| Step | Action | Verification |
|------|--------|--------------|
| 1. Spec | Document in SPEC.md before coding | PR includes spec update |
| 2. Implement | Write code following conventions below | Code review |
| 3. Test | Add tests that run in container | `make test-docker` passes |
| 4. Document | Update CLAUDE.md if user-facing | CLAUDE.md reflects new capability |

**No feature is complete until all four steps pass.**

#### Test-First Workflow

```bash
# 1. Start container environment
make test-docker-shell

# 2. Write failing test first
vim tests/shell/new_feature.bats

# 3. Run test (should fail)
bats tests/shell/new_feature.bats

# 4. Implement feature
vim scripts/new_feature.sh

# 5. Run test (should pass)
bats tests/shell/new_feature.bats

# 6. Run full suite before commit
make test-shell
```

#### Code Conventions

| Area | Convention | Rationale |
|------|------------|-----------|
| Datahub access | All operations via `dh` CLI | Single interface, testable |
| Windmill flows | One folder per source adapter | Clear ownership |
| Shell scripts | Must pass `shellcheck` | Catch common errors |
| New CLIs | Add to CLAUDE.md scripts table | Discoverability |
| Config | Environment vars via BWS | No secrets in code |
| Logging | Structured JSON to stderr | Machine-parseable |

#### Naming Conventions

| Type | Pattern | Example |
|------|---------|---------|
| Scripts | `kebab-case.sh` | `auth-keeper.sh` |
| Windmill flows | `source_action` | `telegram_sync` |
| BWS secrets | `kebab-case` | `aws-cross-accounts` |
| Test files | `feature.bats` | `auth-keeper.bats` |

#### CI Gate Requirements

Nothing merges to main without:

```yaml
# .github/workflows/ci.yml gates
required_checks:
  - shellcheck           # All .sh files
  - terraform validate   # Infrastructure
  - bats tests           # Shell unit tests
  - docker build         # Container builds
```

**PR merge blocked if any check fails.**

#### When to Add vs. Extend

| Situation | Action |
|-----------|--------|
| New data source | Add adapter in `lib/intake/adapters/` |
| New CLI command | Extend existing CLI (e.g., `dh`) not new binary |
| New auth provider | Add to `auth-keeper.sh` lazy-load pattern |
| New backup target | Add to existing backup scripts |
| New triage rule | Add to entity rules, not new triage path |

**Prefer extending existing patterns over creating new ones.**

### 10.8 Executable Verification Pattern

Trust comes from automated verification, not faith. This pattern ensures the spec matches implementation and data is protected.

#### Core Principle

> "If I can't see proof it's working, I add a check that shows me."

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SPEC (what should exist)                                   â”‚
â”‚      â†“                                                      â”‚
â”‚  TESTS (executable checks)                                  â”‚
â”‚      â†“                                                      â”‚
â”‚  VERIFICATION (automated, visible)                          â”‚
â”‚      â†“                                                      â”‚
â”‚  CONFIDENCE (trust through proof)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Executable Spec Format

Each spec section includes testable requirements:

```markdown
## Requirement: Windmill MVP Folders

### Acceptance Criteria
- [ ] Folder `f/sdp/` exists with sync.ts
- [ ] Folder `f/ms365/` exists with sync.ts
- [ ] Folder `f/slack/` exists with sync.ts
- [ ] Folder `f/telegram/` exists with sync.ts
- [ ] Folder `f/ops/` exists with backup.ts

### Verification Command
\`\`\`bash
make verify-windmill
\`\`\`
```

#### Verification Test Structure

```
tests/
â”œâ”€â”€ spec-compliance/
â”‚   â”œâ”€â”€ windmill.bats      # Windmill folder/script checks
â”‚   â”œâ”€â”€ datahub.bats       # Datahub structure checks
â”‚   â”œâ”€â”€ auth.bats          # OAuth configuration checks
â”‚   â””â”€â”€ hooks.bats         # PAI hook installation checks
â”œâ”€â”€ data-safety/
â”‚   â”œâ”€â”€ git-commits.bats   # Git auto-commit working
â”‚   â”œâ”€â”€ snapshots.bats     # LUKS snapshots exist
â”‚   â””â”€â”€ backups.bats       # S3 backups current
â””â”€â”€ run-all.sh             # Full verification suite
```

#### Example Tests

**Spec compliance test (`tests/spec-compliance/windmill.bats`):**

```bash
#!/usr/bin/env bats

@test "sdp folder exists in Windmill" {
  run windmill script list -f json | jq -e '.[] | select(.path | startswith("f/sdp/"))'
  [ "$status" -eq 0 ]
}

@test "sdp/sync.ts script exists" {
  run windmill script exists f/sdp/sync
  [ "$status" -eq 0 ]
}

@test "ms365 OAuth resource configured" {
  run windmill resource list -f json | jq -e '.[] | select(.path == "u/ms365_oauth")'
  [ "$status" -eq 0 ]
}

@test "datahub index exists" {
  [ -f /data/work/datahub/index.sqlite ]
}

@test "dh CLI responds" {
  run dh --version
  [ "$status" -eq 0 ]
}
```

**Data safety test (`tests/data-safety/git-commits.bats`):**

```bash
#!/usr/bin/env bats

@test "git auto-commit ran within last hour" {
  last_commit=$(git log -1 --format=%ct 2>/dev/null || echo 0)
  now=$(date +%s)
  age=$((now - last_commit))
  [ "$age" -lt 3600 ]  # Less than 1 hour
}

@test "LUKS snapshot exists within last 2 hours" {
  latest=$(aws ec2 describe-snapshots \
    --filters "Name=tag:Name,Values=hall-of-fire*" \
    --query 'Snapshots | sort_by(@, &StartTime) | [-1].StartTime' \
    --output text)
  snapshot_time=$(date -d "$latest" +%s)
  now=$(date +%s)
  age=$((now - snapshot_time))
  [ "$age" -lt 7200 ]  # Less than 2 hours
}

@test "datahub has items" {
  count=$(ls /data/work/datahub/items/ 2>/dev/null | wc -l)
  [ "$count" -gt 0 ]
}
```

#### Verification Commands

```bash
# Full verification suite
make verify-all

# Spec compliance only
make verify-spec

# Data safety only
make verify-data

# Quick status (no tests, just checks)
verify-status
```

#### Makefile Targets

```makefile
.PHONY: verify-all verify-spec verify-data

verify-all: verify-spec verify-data
	@echo "âœ“ All verification passed"

verify-spec:
	@echo "Checking spec compliance..."
	@bats tests/spec-compliance/

verify-data:
	@echo "Checking data safety..."
	@bats tests/data-safety/
```

#### Quick Status Command

For immediate peace of mind without running full tests:

```bash
#!/bin/bash
# ~/bin/verify-status

echo "=== Imladris Status ==="
echo ""

# Git status
echo -n "Git: "
if git_time=$(git log -1 --format=%cr 2>/dev/null); then
  echo "âœ“ Last commit $git_time"
else
  echo "âœ— Not in git repo"
fi

# Session capture
echo -n "Session: "
if session=$(ls -t ~/.claude/MEMORY/sessions/*.md 2>/dev/null | head -1); then
  echo "âœ“ $(basename $session)"
else
  echo "âš  No sessions captured"
fi

# Datahub
echo -n "Datahub: "
if [ -d /data/work/datahub/items ]; then
  count=$(ls /data/work/datahub/items/ | wc -l)
  echo "âœ“ $count items"
else
  echo "âœ— Not found"
fi

# Windmill
echo -n "Windmill: "
if curl -s http://localhost:8000/api/version > /dev/null 2>&1; then
  echo "âœ“ Running"
else
  echo "âœ— Not responding"
fi

# LUKS
echo -n "LUKS: "
if mountpoint -q /data 2>/dev/null; then
  echo "âœ“ Mounted"
else
  echo "âœ— Not mounted"
fi

echo ""
echo "Run 'make verify-all' for full verification"
```

#### PAI Integration

**Stop hook runs verification:**

```typescript
// hooks/VerifyOnStop.hook.ts
import { execSync } from 'child_process';

export default async function verifyOnStop() {
  try {
    // Quick verification (non-blocking)
    execSync('verify-status', { timeout: 5000 });
  } catch (e) {
    // Log failure but don't block
    console.error('Verification warning:', e.message);
  }
}
```

**Failures captured in MEMORY:**

```
~/.claude/MEMORY/
â”œâ”€â”€ verification/
â”‚   â”œâ”€â”€ 2026-02-11-passed.md
â”‚   â”œâ”€â”€ 2026-02-10-failed.md  # Contains what failed
â”‚   â””â”€â”€ history.jsonl          # All results
```

**Next session context:**

```
SessionStart hook checks MEMORY/verification/
If recent failure â†’ inject into context:
"âš ï¸ Last session: verify-spec failed - sdp/sync.ts missing"
```

#### Visual Feedback

**Tmux status line indicator:**

```bash
# In ~/.tmux.conf
set -g status-right '#(verify-status-short) | %H:%M'
```

```bash
# ~/bin/verify-status-short
if git log -1 --format=%ct 2>/dev/null | \
   awk -v now=$(date +%s) '{exit ($1 > now-3600) ? 0 : 1}'; then
  echo "âœ“"
else
  echo "âš "
fi
```

**What you see:**
- `âœ“` = All good, committed within last hour
- `âš ` = Warning, check status
- `âœ—` = Problem, run verification

#### Verification Cadence

| Check | When | How |
|-------|------|-----|
| `verify-status` | On demand | Manual or prompt |
| Status line | Always visible | Passive indicator |
| `verify-spec` | After implementation | Manual or CI |
| `verify-data` | Daily | Cron job |
| Full suite | Before major changes | Manual |

#### Trust Loop

```
You work
    â†“
Status line shows âœ“ (git committed)
    â†“
Stop hook runs verify-status
    â†“
Any failure â†’ logged to MEMORY
    â†“
Next session â†’ warned if issues
    â†“
You know exactly what's verified
```

**The result:** You never wonder. You always know.

### 10.9 Agent Teams for Fast Parallel Development

PAI v3.0 provides native agent team support for large coding projects requiring parallel execution.

#### Three Agent Systems in PAI v3.0

| System | Purpose | When to Use |
|--------|---------|-------------|
| **Task Tool** | Parallel independent work | Research, file exploration, simple parallel tasks |
| **Named Agents** | Persistent identities with voices | Recurring work, voice output, relationships |
| **Agent Teams** | Coordinated swarm execution | Large features, tight deadlines, complex collaboration |

#### Enable Agent Teams

```json
// ~/.claude/settings.json
{
  "env": {
    "CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS": "1"
  }
}
```

#### How Algorithm v1.4.0 + Agent Teams Work Together

```
Algorithm (v1.4.0)                Agent Teams (Execution)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OBSERVE: Extract constraints â”€â”€â”€â–º (solo - criteria generation)
THINK: Pressure test ISC     â”€â”€â”€â–º (solo - criteria refinement)
                                  â”‚
PLAN: Parallelization        â”€â”€â”€â–º If 3+ independent criteria:
assessment                        â”‚   "create an agent team"
                                  â”‚   â†’ Partition into domains
                                  â”‚   â†’ Spawn N agents (1 per domain)
                                  â”‚
BUILD: ISC adherence check   â”€â”€â”€â–º Teammates work in parallel:
                                  â”œâ”€â”€ Engineer A (auth module)
                                  â”œâ”€â”€ Engineer B (API layer)
                                  â”œâ”€â”€ Engineer C (UI components)
                                  â””â”€â”€ QATester (writes tests)
                                  â”‚
                                  â”‚   Each agent:
                                  â”‚   - Re-reads ISC before artifacts
                                  â”‚   - Checks anti-criteria after
                                  â”‚   - Discusses blockers with team
                                  â”‚
VERIFY: Drift check          â”€â”€â”€â–º Spotcheck agent validates
LEARN: Capture to MEMORY     â”€â”€â”€â–º Reflections to JSONL
```

**Magic phrase:** Say "create an agent team" to invoke PAI's native team system.

#### Loop Mode: Unbounded Parallel Workers

For very large tasks (100+ ISC criteria), use loop mode:

```bash
# External invocation
algorithm.ts -m loop -a 8    # 8 parallel workers
```

| Parameter | Purpose |
|-----------|---------|
| `-m loop` | Loop mode (unbounded iterations) |
| `-a N` | Number of parallel workers |

**Behavior:**
- Criteria distributed across N workers
- Effort decay: early iterations Extended â†’ later iterations Fast
- Convergence tracking via PRD frontmatter
- Status: `loopStatus: running|paused|completed|failed`

#### Named Agents (13 Available)

PAI v3.0 includes named agents with persistent identities:

| Agent | Specialty | Voice |
|-------|-----------|-------|
| Serena Blackwood | Security architecture | ElevenLabs mapped |
| Marcus Webb | Research synthesis | ElevenLabs mapped |
| Rook Blackburn | Adversarial analysis | ElevenLabs mapped |
| Dev Patel | Engineering | ElevenLabs mapped |
| Ava Sterling | UX/design | ElevenLabs mapped |
| Alex Rivera | Full-stack development | ElevenLabs mapped |
| PerplexityResearcher | Web research | ElevenLabs mapped |
| ... | (6 more) | ... |

**Use for:** Recurring interactions, voice output, building relationships across sessions.

#### When to Use Each Approach

| Approach | Best For | Trigger |
|----------|----------|---------|
| **Task Tool** | Parallel independent work | Multiple Task calls in single message |
| **Named Agents** | Personality, voice, relationships | Reference by name |
| **Agent Teams** | Complex collaborative work | Say "create an agent team" |
| **Loop Mode** | Very large tasks (100+ criteria) | `algorithm.ts -m loop` |

**Decision criteria:**
- Just need parallel results â†’ Task tool
- Agents need to discuss/debate â†’ Agent Teams
- Building large feature fast â†’ Agent Teams
- Massive scale (100+ criteria) â†’ Loop Mode
- Want personality/voice â†’ Named Agents

#### Practical Workflow

**Step 1: Start with Algorithm planning**

```
Build a full-stack ticket management system with auth, API, and dashboard
```

Algorithm runs OBSERVE â†’ THINK â†’ PLAN phases solo, generating ISC.

**Step 2: PLAN phase identifies parallelization opportunity**

When Algorithm reaches PLAN and finds 3+ independent criteria domains, it outputs:

```
PARALLELIZATION ASSESSMENT:
â”‚ Independent domains: auth, api, ui, tests
â”‚ Criteria per domain: 4, 5, 3, 4
â”‚ Recommendation: Create an agent team
```

**Step 3: Spawn team with domain assignments**

```
Create an agent team to implement this plan:
- Lead (opus): Architecture decisions, conflict resolution
- Engineer-Auth (sonnet): Auth module (criteria 1-4)
- Engineer-API (sonnet): REST endpoints (criteria 5-9)
- Engineer-UI (sonnet): React components (criteria 10-12)
- QATester (sonnet): Write tests as engineers complete (criteria 13-16)

Each teammate follows BUILD phase:
- Re-read ISC criteria before each artifact
- Check anti-criteria after each artifact
- Flag blockers to team via shared task list
```

**Step 4: Monitor and steer**

| Keys | Action |
|------|--------|
| `Shift+Tab` | Toggle delegate mode (lead coordinates only) |
| `Ctrl+T` | View shared task list |
| `Shift+Up/Down` | Select teammates to message |

#### Model Selection for Teams

| Role | Model | Rationale |
|------|-------|-----------|
| Lead (coordinator) | opus | Strategic decisions, conflict resolution |
| Engineers | sonnet | Fast coding with good quality |
| QATester | sonnet | Needs to understand code deeply |
| Researchers | haiku | Fast parallel lookups |

#### What PAI v3.0 Provides for Teams

| PAI Provides | Agent Teams Provides |
|--------------|---------------------|
| ISC criteria (what "done" means) | Shared task list |
| Constraint extraction (rules from requirements) | Inter-agent communication |
| Build drift prevention (re-read before artifact) | Parallel execution |
| PRDs (context survives sessions) | Self-coordination |
| Named agents (personality, voice) | â€” |
| Loop mode (unbounded parallelization) | â€” |
| MEMORY (learnings, ratings) | â€” |

#### Spotcheck Pattern

**Always launch spotcheck agent after parallel work:**

```
Spotcheck: Verify all team work against ISC criteria
- Check for drift between agents
- Validate integration points
- Confirm anti-criteria not violated
```

#### Comparison: Execution Approaches

| Factor | Task Tool | Agent Teams | Loop Mode |
|--------|-----------|-------------|-----------|
| **Setup** | None | `EXPERIMENTAL_AGENT_TEAMS=1` | External CLI |
| **Communication** | Results return to main | Agents message each other | Independent workers |
| **Coordination** | Main orchestrates | Shared task list | PRD-based convergence |
| **Scale** | 2-10 parallel | 2-10 agents | Unbounded (8+ workers) |
| **Best for** | Simple parallel | Collaborative | Massive tasks |
| **Token cost** | Lower | Higher | Highest |

#### Recommended Usage in Imladris

| Scenario | Approach |
|----------|----------|
| Default work | Solo Algorithm execution |
| Parallel research | Task tool with multiple calls |
| Large features | Agent Teams |
| Tight deadlines | Agent Teams with aggressive parallelization |
| Very large projects (100+ criteria) | Loop mode |
| Personality/recurring interactions | Named agents |

---

## 11. Mobile Access (OpenClaw)

### 11.1 Purpose

Secondary access to Claude from iOS/Android via existing chat platforms. [OpenClaw](https://github.com/openclaw/openclaw) is an open-source AI gateway that bridges messaging platforms with Claude.

#### Primary vs Mobile Access

| | Primary (Claude Code) | Mobile (OpenClaw) |
|---|----------------------|-------------------|
| **Interface** | tmux + Claude CLI | Telegram/WhatsApp/Signal |
| **Use case** | Active development work | Quick check-ins, status, simple tasks |
| **Context** | Full interactive session | Shared filesystem context |
| **Frequency** | Daily, hours at a time | Occasional, minutes at a time |

**tmux remains the primary interface.** OpenClaw is for mobile convenience only.

#### Why OpenClaw vs Custom Implementation

| Factor | OpenClaw | Custom tmux Bridge |
|--------|----------|-------------------|
| Maintenance | Active open-source project (180k+ stars) | Custom shell scripts |
| Platform support | Telegram, WhatsApp, Signal, Discord, iMessage, Slack | Telegram only |
| Response handling | Native async with proper streaming | Sentinel pattern hacks |
| Image support | Built-in multimodal | Not supported |
| Reliability | Production-grade | Best-effort |

### 11.2 Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Mobile Device (iOS/Android)                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Telegram    â”‚  â”‚ WhatsApp    â”‚  â”‚ Signal      â”‚  ...    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                â”‚                â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  imladris                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ OpenClaw (Docker)                                    â”‚   â”‚
â”‚  â”‚  - Multi-platform gateway                            â”‚   â”‚
â”‚  â”‚  - Message routing & streaming                       â”‚   â”‚
â”‚  â”‚  - Image/file handling                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                        â”‚                                    â”‚
â”‚                        â–¼ Anthropic API                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Claude (via Bedrock or API key)                      â”‚   â”‚
â”‚  â”‚  - Same model as Claude Code                         â”‚   â”‚
â”‚  â”‚  - Shared context via MCP/filesystem                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key difference from Claude Code sessions:** OpenClaw runs its own Claude instance via API, not the interactive Claude Code CLI. Context sharing happens through the filesystem and MCP servers, not tmux.

### 11.3 Configuration

**Docker Compose setup:**

```yaml
# docker-compose.openclaw.yml
services:
  openclaw:
    image: openclaw/openclaw:latest
    container_name: openclaw
    restart: unless-stopped
    volumes:
      - /data/openclaw:/app/data
      - /data/datahub:/context/datahub:ro  # Shared context
    environment:
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}  # Or use Bedrock
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}
      - ALLOWED_CHAT_IDS=${TELEGRAM_CHAT_ID}
      - MODEL=claude-sonnet-4-20250514
    ports:
      - "127.0.0.1:3080:3080"  # Web UI (optional)
```

**Secrets (BWS):**

| Secret | Purpose |
|--------|---------|
| `telegram-bot-token` | Telegram Bot API token |
| `telegram-chat-id` | Authorized chat ID(s) |
| `anthropic-api-key` | API key (if not using Bedrock) |

### 11.4 Commands

From any connected platform (Telegram, WhatsApp, etc.):

| Command | Action |
|---------|--------|
| Direct message | Chat with Claude |
| `/status` | Show OpenClaw status |
| `/model` | Show/switch model |
| `/clear` | Clear conversation context |
| `/help` | List available commands |

**Note:** OpenClaw handles conversation context nativelyâ€”no `/c` prefix needed.

### 11.5 Context Sharing

OpenClaw runs a separate Claude instance, but can share context with Claude Code sessions:

**Filesystem context (read-only mounts):**

| Path | Purpose |
|------|---------|
| `/context/datahub` | Current tasks, items, notes |
| `/context/repos` | Code repositories |
| `/context/inbox` | Incoming messages |

**MCP servers:** OpenClaw can connect to the same MCP servers as Claude Code for tool access.

**System prompt:** Configure OpenClaw's system prompt to reference imladris context:

```
You are an assistant with access to the imladris datahub.
Current tasks are in /context/datahub/items/.
Reference these when answering questions about work status.
```

### 11.6 Security

| Control | Implementation |
|---------|----------------|
| Auth | `ALLOWED_CHAT_IDS` env var (comma-separated) |
| Rate limit | OpenClaw built-in rate limiting |
| Cost awareness | OpenClaw token tracking & limits |
| API key | Use dedicated API key, not personal subscription |

**Important:** Do not use Claude Pro/Max access tokensâ€”this violates Anthropic ToS. Use a pay-as-you-go API key.

### 11.7 Limitations

| Limitation | Notes |
|------------|-------|
| Separate context | Not the same session as Claude Code |
| API costs | Pay-per-use (not included in subscriptions) |
| No direct file editing | Can read context, but edits go through Claude Code |

### 11.8 Example Session

```
You: What's the status of the auth bug?

Claude: Looking at the datahub context...

SDP-1234 (auth bug) is in progress. From the latest notes:
- Retry logic added to token refresh
- Tests passing locally
- CI pending

The item is assigned to you with P1 priority.

---

You: [sends screenshot of error message]

Claude: I can see the error in your screenshot. The issue is
a null pointer in the token validation. Based on the code
in /context/repos/auth-service, you need to add a null
check on line 142 of TokenValidator.ts.

---

You: Thanks, I'll fix it when I'm back at my desk

Claude: Sounds good. Want me to add a note to the ticket
about the fix needed?

---

You: Yes please

Claude: Done. Added note to SDP-1234:
"Fix needed: null check on TokenValidator.ts:142"
```

### 11.9 Implementation

| Step | Action |
|------|--------|
| 1 | Create Telegram bot via BotFather |
| 2 | Store tokens in BWS |
| 3 | Deploy OpenClaw via Docker Compose |
| 4 | Configure context mount paths |
| 5 | Test from mobile device |

**Startup:** OpenClaw starts automatically via Docker Compose on boot.

---

## 12. Out of Scope

| Feature | Reason |
|---------|--------|
| Outbound automation | Collector, not actor |
| Push notifications | Polling model |
| Multi-instance/HA | Single user workstation |
| Native mobile app | OpenClaw via Telegram/WhatsApp/Signal instead |
| GUI | Terminal only (Telegram is the "GUI") |
| Offline Claude | Bedrock requires network |

---

## 13. Open Questions

| Question | Status |
|----------|--------|
| Secondary offsite backup destination | S3 (see Appendix F) |
| Attachment storage/download location | On-demand to `/data/attachments/` |
| Triage feedback loop (improve Claude) | v2 |
| Mobile/multi-device access | âœ“ Resolved: OpenClaw (Section 11) |

---

## Appendix A: Field Sync Matrix

### ServiceDesk Plus

| Field | Inbound | Outbound |
|-------|---------|----------|
| id | âœ“ | â€” |
| title/subject | âœ“ | âœ“ |
| description | âœ“ | âœ“ (create) |
| status | âœ“ | âœ“ |
| priority | âœ“ | âœ“ |
| assignee | âœ“ | âœ“ |
| requester | âœ“ | â€” |
| notes | âœ“ | âœ“ (append) |
| worklogs | âœ“ | âœ“ (append) |
| resolution | âœ“ | âœ“ (close) |

### Azure DevOps

| Field | Inbound | Outbound |
|-------|---------|----------|
| id | âœ“ | â€” |
| title | âœ“ | âœ“ |
| description | âœ“ | âœ“ |
| state | âœ“ | âœ“ |
| assigned_to | âœ“ | âœ“ |
| priority | âœ“ | âœ“ |
| comments | âœ“ | âœ“ (append) |

### MS365 Mail

| Field | Inbound | Outbound |
|-------|---------|----------|
| id, subject, from, to, body | âœ“ | â€” |
| is_read, categories, flag | âœ“ | âœ“ |
| Send reply | â€” | âœ“ |

### Slack

| Field | Inbound | Outbound |
|-------|---------|----------|
| id, channel, author, text | âœ“ | â€” |
| Post reply, add reaction | â€” | âœ“ |

### Gmail / GCal / Telegram

Similar patterns - see detailed field matrix in conversation.

---

## Appendix B: Polling Schedule

| Poller | Interval | Method |
|--------|----------|--------|
| ms365-mail | 5 min | Delta query |
| ms365-cal | 5 min | Delta query |
| sdp | 5 min | Modified time filter |
| devops | 5 min | Work item query |
| slack | 60s | slackdump |
| gmail | 5 min | history.list |
| gcal | 5 min | syncToken |
| telegram | 60s | getUpdates |

---

## Appendix C: PAI Integration

Imladris 2.0 builds around PAI v3.0 (Personal AI Infrastructure):

**PAI v3.0 Provides:**

| Category | Components |
|----------|------------|
| **Skills** | 37 production skills (Research, Science, Browser, Evals, etc.) |
| **Hooks** | 20 lifecycle hooks across 6 events |
| **Agents** | 13 named agents + dynamic composition + agent teams |
| **Algorithm** | v1.4.0 with ISC, constraint extraction, drift prevention |
| **MEMORY** | 7-tier system (WORK, LEARNING, RESEARCH, STATE, etc.) |
| **PRDs** | Persistent requirements documents surviving sessions |
| **Thinking** | Council, RedTeam, FirstPrinciples, Science, BeCreative, IterativeDepth |
| **Parallel** | Loop mode (N workers), agent teams, fan-out patterns |

**New in v3.0 (vs v2.5):**

| Feature | Benefit |
|---------|---------|
| **Persistent PRDs** | Task context survives sessions |
| **Agent Teams** | Native swarm coordination with shared task lists |
| **Loop Mode** | Unbounded parallel workers on ISC criteria |
| **Constraint Extraction** | Mechanical extraction of rules from requirements |
| **Build Drift Prevention** | Re-read ISC before each artifact |
| **Verification Rehearsal** | Simulate failures before real verification |
| **Context Recovery** | â‰¤34s cold-start recovery from MEMORY/PRDs |
| **Skill-Index** | 75% context savings via deferred loading |

**Imladris Provides:**

| Component | Purpose |
|-----------|---------|
| **5-Zone Workflow** | Workspace organization (work/home/research/tasks/adhoc) |
| **Datahub** | External sources â†’ unified task storage |
| **Windmill** | External API gateway (SDP, MS365, Slack polling) |
| **Bidirectional Sync** | Write-back to ServiceDesk Plus |
| **OpenClaw** | Mobile access via Telegram/WhatsApp/Signal |
| **Context Signaling** | Status bar, zone colors |

### C.1 PAI Hooks to Adopt (20 Available)

**SessionStart (3 hooks):**

| Hook | Purpose | Imladris Benefit |
|------|---------|------------------|
| `StartupGreeting.hook.ts` | Personalized greeting | Consistent session start |
| `LoadContext.hook.ts` | Inject PAI SKILL.md | No manual context loading |
| `CheckVersion.hook.ts` | Version compatibility | Auto-detect PAI updates |

**UserPromptSubmit (4 hooks):**

| Hook | Purpose | Imladris Benefit |
|------|---------|------------------|
| `RatingCapture.hook.ts` | Capture 1-10 ratings + Haiku inference | Feedback without prompting |
| `AutoWorkCreation.hook.ts` | Create work directories, track ISC | Automatic task organization |
| `UpdateTabTitle.hook.ts` | Dynamic tab titles | Context visibility |
| `SessionAutoName.hook.ts` | Auto-name sessions | Better session management |

**PreToolUse (4 hooks):**

| Hook | Purpose | Imladris Benefit |
|------|---------|------------------|
| `SecurityValidator.hook.ts` | Block dangerous commands via patterns.yaml | Security without custom code |
| `SetQuestionTab.hook.ts` | Tab state on questions | UX clarity |
| `AgentExecutionGuard.hook.ts` | Security constraints on multi-agent | Safe parallel execution |
| `SkillGuard.hook.ts` | Skill access control | Skill isolation |

**PostToolUse (2 hooks):**

| Hook | Purpose | Imladris Benefit |
|------|---------|------------------|
| `AlgorithmTracker.hook.ts` | Phase tracking + ISC mutations | Algorithm observability |
| `QuestionAnswered.hook.ts` | Track question resolution | UX completion |

**Stop (1 hook):**

| Hook | Purpose | Imladris Benefit |
|------|---------|------------------|
| `StopOrchestrator.hook.ts` | Coordinate voice + tab resets | Unified stop handling |

**SessionEnd (6 hooks):**

| Hook | Purpose | Imladris Benefit |
|------|---------|------------------|
| `WorkCompletionLearning.hook.ts` | Extract algorithm learnings | Continuous improvement |
| `SessionSummary.hook.ts` | Mark work COMPLETED | Auto-capture to datahub |
| `RelationshipMemory.hook.ts` | Track user preferences | Personalization |
| `UpdateCounts.hook.ts` | Update system statistics | Metrics |
| `IntegrityCheck.hook.ts` | System health validation | Reliability |
| `ImplicitSentimentCapture.hook.ts` | Haiku sentiment analysis | Detect frustration |

### C.2 Custom Imladris Hooks

| Hook | Event | Purpose | Why Custom |
|------|-------|---------|------------|
| `DatahubSync.hook.ts` | SessionEnd | Sync completed tasks to datahub | Datahub-specific logic |
| `ZoneContext.hook.ts` | SessionStart | Load zone-specific context | 5-zone system is imladris-specific |
| `OpenClawContext.hook.ts` | SessionEnd | Sync context for mobile access | OpenClaw reads shared filesystem |
| `WindmillTrigger.hook.ts` | SessionEnd | Trigger Windmill sync jobs | External API coordination |

### C.3 What PAI Does NOT Replace

| Component | Why Still Needed |
|-----------|------------------|
| **Windmill** | PAI has no external API gateway - Windmill handles Slack/SDP/Telegram polling |
| **Datahub** | PAI MEMORY is for learnings; datahub is task source of truth |
| **OpenClaw** | PAI has no mobile access mechanism - OpenClaw provides it |
| **5-Zone Workflow** | PAI has TELOS (goals) but not workspace organization |
| **`dh` CLI** | PAI doesn't provide unified task/item interface |
| **Bidirectional SDP Sync** | PAI doesn't integrate with ServiceDesk Plus |

### C.4 PRD Integration with Datahub

PAI v3.0's Persistent PRDs complement datahub:

| System | Purpose | Data Type |
|--------|---------|-----------|
| **Datahub** | Task source of truth | Items from SDP, Slack, email |
| **PRDs** | Execution context | ISC criteria, decisions, progress |

**Integration pattern:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Datahub Item (source of truth)                             â”‚
â”‚  /data/work/datahub/items/sdp-1234.md                       â”‚
â”‚  - What: "Fix auth token refresh"                           â”‚
â”‚  - Source: SDP ticket                                       â”‚
â”‚  - Status: in-progress                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼ Claude starts work
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PRD (execution context)                                    â”‚
â”‚  ~/.claude/MEMORY/WORK/auth-fix/PRD-20260217-auth-fix.md   â”‚
â”‚  - ISC: 12 criteria for "done"                              â”‚
â”‚  - Progress: 8/12 passing                                   â”‚
â”‚  - Decisions: JWT over sessions (stateless)                 â”‚
â”‚  - Survives session restart                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼ Work completes
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Datahub Item (updated)                                     â”‚
â”‚  - Status: done                                             â”‚
â”‚  - Link to PRD for audit trail                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**PRD frontmatter fields:**

```yaml
prd: true
id: PRD-{YYYYMMDD}-{slug}
status: DRAFT|CRITERIA_DEFINED|PLANNED|IN_PROGRESS|VERIFYING|COMPLETE
effort_level: Instant|Fast|Standard|Extended|Deep|Comprehensive|Loop
iteration: 0
last_phase: OBSERVE|THINK|PLAN|BUILD|EXECUTE|VERIFY|LEARN
verification_summary: "8/12"
datahub_item: sdp-1234  # Link back to source
```

### C.5 settings.json Configuration

Enable PAI v3.0 features in `~/.claude/settings.json`:

```json
{
  "env": {
    "CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS": "1",
    "CLAUDE_CODE_MAX_OUTPUT_TOKENS": "80000"
  },
  "permissions": {
    "allow": ["Bash", "Read", "Write", "Edit", "Skill", "Task", "mcp__*"]
  },
  "hooks": {
    "SessionStart": ["StartupGreeting", "LoadContext", "ZoneContext"],
    "UserPromptSubmit": ["RatingCapture", "AutoWorkCreation"],
    "PreToolUse": ["SecurityValidator", "AgentExecutionGuard"],
    "PostToolUse": ["AlgorithmTracker"],
    "Stop": ["StopOrchestrator"],
    "SessionEnd": ["WorkCompletionLearning", "SessionSummary", "DatahubSync"]
  }
}
```

**Layer Integration:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Request                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spec Kit: /specify                                             â”‚
â”‚  "WHAT to build" - requirements, constraints, acceptance        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PAI: Skills + TELOS                                            â”‚
â”‚  "HOW to think" - methodology, patterns, goal alignment         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLAUDE.md: Project Rules                                       â”‚
â”‚  "HOW to behave here" - repo conventions, constraints           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Claude Code: Execution                                         â”‚
â”‚  "DO the work" - code, tests, commits                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spec Kit: /verify                                              â”‚
â”‚  "CONFIRM done" - acceptance criteria validation                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Curu Skills (PAI) for Imladris:**

| Category | Skills |
|----------|--------|
| **Integration** (thin routing) | One skill per Windmill module |
| **AI Processing** | Triage, SpecAssist, TaskContext, Comms |

**Integration skills** â€” one per Windmill folder:

| Skill | Routes To | Purpose |
|-------|-----------|---------|
| SDP | `f/sdp/*` | ServiceDesk Plus tickets |
| MS365 | `f/ms365/*` | Email, calendar |
| Slack | `f/slack/*` | Messages |
| AWS | `f/aws/*` | Cross-account, S3 |
| Ramp | `f/ramp/*` | Expenses |
| Securonix | `f/securonix/*` | Security alerts |
| Ops | `f/ops/*` | Backups, maintenance |

**AI Processing skills:**

| Skill | Purpose |
|-------|---------|
| Triage | Batch classification |
| SpecAssist | Spec writing |
| TaskContext | Context summarization |
| Comms | Reply drafting |

**Pattern:** Each integration skill is ~20 lines of routing. Logic lives in Windmill scripts.

```
User: "show my SDP tickets"
  â†’ SDP skill (routing)
  â†’ Windmill f/sdp/get-tickets.ts (logic)
  â†’ Returns data
```

---

## Appendix D: Host vs AI Boundary

### Overview

Clear separation between deterministic host code (Imladris) and AI-interpreted skills (Curu-skills).

| Layer | Repository | Characteristics |
|-------|------------|-----------------|
| **Host (Imladris)** | sethdf/imladris | Deterministic, versioned, tested, CLI/code |
| **AI (Curu-skills)** | sethdf/curu-skills | AI-interpreted, flexible, customizable |

### Host Layer (Imladris)

Deterministic functionality that must work identically every time:

| Component | Implementation | Why Host |
|-----------|----------------|----------|
| Windmill scripts | Bun/Python | Deterministic sync/API logic |
| Workspace commands | Shell | Deterministic tmux control |
| Status TUI | Python | Deterministic display |
| Datahub CLI | Bun | Deterministic data operations |
| Bootstrap | Shell | System initialization |

**Note:** Pollers, queue processor, and auth-keeper are now Windmill scripts, not separate components.

### AI Layer (Curu-skills)

Functionality that benefits from AI interpretation:

| Component | Skill | Why AI |
|-----------|-------|--------|
| Triage | Triage skill | Classification requires understanding |
| Context summarization | TaskContext skill | Summarization is AI strength |
| Reply drafting | Comms skill | Composition benefits from AI |
| Search interpretation | Search skill | Intent understanding |
| Task breakdown | Task skill | Planning benefits from AI |
| Priority suggestions | Triage skill | Judgment calls |

### Decision Rule

| If the component... | Then... |
|---------------------|---------|
| Must work identically every time | Host (Imladris) |
| Benefits from AI interpretation | AI (Curu-skills) |
| User might want to customize behavior | AI (Curu-skills) |
| Is security/auth sensitive | Host (Imladris) |
| Directly interacts with external APIs | Host (Imladris) |
| Formats, interprets, or generates content | AI (Curu-skills) |

### Interface Boundary

Skills call Imladris CLI tools, never external APIs directly:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Layer (Curu-skills)                                         â”‚
â”‚                                                                 â”‚
â”‚  Triage skill â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  Task skill â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–º Calls Imladris CLI                     â”‚
â”‚  Comms skill â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Host Layer (Imladris)                                          â”‚
â”‚                                                                 â”‚
â”‚  datahub query --actionable     â† Read items                    â”‚
â”‚  datahub write --note "..."     â† Write to queue                â”‚
â”‚  datahub triage --batch         â† Trigger triage                â”‚
â”‚  Windmill API                   â† Credentials (internal only)   â”‚
â”‚                                                                 â”‚
â”‚                    â”‚                                            â”‚
â”‚                    â–¼                                            â”‚
â”‚            External APIs (SDP, MS365, etc.)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Cloud Account Discoverability

Skills need to know which cloud accounts are accessible without accessing credentials directly.

**Problem:** Claude doesn't inherently know which AWS/GCP accounts exist or are accessible.

**Solution:** Skills call Windmill script to get account registry:

```bash
# Skill calls Windmill to discover accounts
curl -s http://localhost:8000/api/w/main/jobs/run_wait_result/f/aws/list-accounts | jq

# Returns:
{
  "accounts": [
    {
      "id": "111111111111",
      "name": "prod",
      "alias": "org-prod",
      "purpose": "Production environment",
      "roles": ["ReadOnlyAccess", "AdminAccess"],
      "default_role": "ReadOnlyAccess"
    },
    ...
  ]
}

# Skill calls Windmill to get session credentials
curl -s http://localhost:8000/api/w/main/jobs/run_wait_result/f/aws/get-session \
  -d '{"account": "org-prod", "role": "ReadOnly"}'
# Returns temporary credentials for direct AWS CLI use
```

**Skill Pattern for Cloud Work:**

```markdown
## AWS Account Discovery

Before any AWS operation:
1. Call `f/aws/list-accounts` via Windmill API
2. Present accounts to user if ambiguous
3. Call `f/aws/get-session` to get temporary credentials
4. Proceed with AWS CLI commands using returned credentials

Never assume account IDs or hardcode credentials.
```

**Registry Updates:**

When user gains/loses access to accounts:
1. Update BWS secret (`aws-accounts` or `gcp-projects`)
2. Run `f/ops/bws-sync.ts` (or wait for scheduled sync)
3. Skills automatically discover new accounts on next query

### Benefits

| Benefit | How |
|---------|-----|
| Security | Auth never exposed to AI layer |
| Testability | Host layer is deterministic, testable |
| Flexibility | AI layer can be customized without breaking core |
| Reliability | Critical sync logic is deterministic |
| Upgradability | Can update skills without touching host code |
| Discoverability | Skills query host for available resources |

---

## Appendix E: PAI Workspace Integration

### Overview

Context preservation uses PAI's existing infrastructure (hooks, memory, skills) rather than building parallel systems. Hooks handle session boundaries; skills handle intra-session transitions.

### Context Storage

All context stored within PAI's structure:

```
~/.claude/history/
â”œâ”€â”€ workspaces/                  â† Workspace-level context
â”‚   â”œâ”€â”€ work:tasks.md
â”‚   â”œâ”€â”€ work:projects.md
â”‚   â”œâ”€â”€ work:comms.md
â”‚   â”œâ”€â”€ home:tasks.md
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tasks/                       â† Task-level context
â”‚   â”œâ”€â”€ sdp-123.md
â”‚   â”œâ”€â”€ sdp-456.md
â”‚   â””â”€â”€ ...
â”œâ”€â”€ sessions/                    â† Existing PAI
â”œâ”€â”€ learnings/
â””â”€â”€ decisions/
```

### Environment Variables

Set by workspace switch commands, read by hooks and skills:

```bash
export CONTEXT=work              # Zone (set by direnv on cd)
export WORKSPACE_MODE=tasks      # Mode (set by /work tasks)
export WORKSPACE_NAME=work:tasks # Combined (derived)
export CURRENT_TASK=sdp-123      # Active task ID
```

### Session Lifecycle (Hooks)

**SessionStart Hook** - Loads context on session start:

```typescript
// ~/.claude/hooks/workspace-context-hook.ts
export async function onSessionStart(input: HookInput): Promise<HookOutput> {
  const workspace = process.env.WORKSPACE_NAME;
  const taskId = process.env.CURRENT_TASK;

  let context = "";

  // Load workspace context
  const workspaceFile = `~/.claude/history/workspaces/${workspace}.md`;
  if (fs.existsSync(workspaceFile)) {
    context += fs.readFileSync(workspaceFile, 'utf-8');
  }

  // Load task context if active
  if (taskId) {
    const taskFile = `~/.claude/history/tasks/${taskId}.md`;
    if (fs.existsSync(taskFile)) {
      context += "\n\n" + fs.readFileSync(taskFile, 'utf-8');
    }
  }

  return { continue: true, context };
}
```

**SessionEnd Hook** - Saves context on session end:

```typescript
// ~/.claude/hooks/workspace-autosave-hook.ts
export async function onSessionEnd(input: HookInput): Promise<HookOutput> {
  const workspace = process.env.WORKSPACE_NAME;
  const taskId = process.env.CURRENT_TASK;

  // Generate summary from transcript
  const summary = generateSummary(input.transcript);

  // Save workspace context
  const workspaceFile = `~/.claude/history/workspaces/${workspace}.md`;
  fs.writeFileSync(workspaceFile, formatWorkspaceContext(summary, taskId));

  // Save task context if active
  if (taskId) {
    const taskFile = `~/.claude/history/tasks/${taskId}.md`;
    fs.writeFileSync(taskFile, formatTaskContext(summary));
  }

  return { continue: true };
}
```

**PreCompact Hook** - Preserves context before summarization:

```typescript
// ~/.claude/hooks/precompact-save-hook.ts
export async function onPreCompact(input: HookInput): Promise<HookOutput> {
  // Same logic as SessionEnd - save before context is compacted
  // Ensures context is preserved even if Claude summarizes aggressively
  saveCurrentContext(input);
  return { continue: true };
}
```

### Context Lock (Race Condition Prevention)

With parallel tmux panes (`/task split`), multiple Claude sessions may attempt to write to the same workspace context file simultaneously.

**Problem:**

```
Pane 1 (sdp-123): auto-save triggers
Pane 2 (sdp-456): auto-save triggers at same time
    â†“
Both write to work:tasks.md
    â†“
Last-write-wins â†’ context from one pane lost
```

**Solution:** File-level advisory locks using `flock`:

```typescript
// ~/.claude/hooks/lib/context-lock.ts
import { execSync } from 'child_process';
import * as fs from 'fs';

export async function withContextLock<T>(
  file: string,
  operation: () => Promise<T>
): Promise<T> {
  const lockFile = `${file}.lock`;
  const fd = fs.openSync(lockFile, 'w');

  try {
    // Acquire exclusive lock (blocks if another process holds it)
    execSync(`flock -x ${fd}`);
    return await operation();
  } finally {
    // Release lock
    fs.closeSync(fd);
  }
}

// Usage in hooks:
export async function onSessionEnd(input: HookInput): Promise<HookOutput> {
  const workspace = process.env.WORKSPACE_NAME;
  const workspaceFile = `~/.claude/history/workspaces/${workspace}.md`;

  await withContextLock(workspaceFile, async () => {
    const summary = generateSummary(input.transcript);
    fs.writeFileSync(workspaceFile, formatWorkspaceContext(summary));
  });

  return { continue: true };
}
```

**Lock behavior:**

| Scenario | Behavior |
|----------|----------|
| Pane 1 writing, Pane 2 tries | Pane 2 blocks until Pane 1 finishes |
| Lock held > 5 sec | Timeout, log warning, proceed anyway |
| Process crashes while holding | Lock auto-released (flock behavior) |

**Files that need locking:**

| File | Why |
|------|-----|
| `workspaces/{workspace}.md` | Multiple panes in same workspace |
| `tasks/{task-id}.md` | Same task open in split panes |

**Files that don't need locking:**

| File | Why |
|------|-----|
| Per-session JSONL | Each session writes to its own file |
| Datahub items | Windmill handles via queue |

**Merge strategy:**

| Content Type | Strategy |
|--------------|----------|
| Logs/notes sections | Append-only (both panes' notes preserved) |
| Current state summaries | Last-write-wins with `.bak` revision |
| Metadata (timestamps, IDs) | Most recent value |

```typescript
// Merge with automatic backup
async function writeContextWithMerge(file: string, newContent: ContextFile) {
  const release = await lock(file, { retries: 5, stale: 5000 });

  try {
    const existing = fs.existsSync(file)
      ? parseContext(fs.readFileSync(file, 'utf-8'))
      : null;

    // Backup before overwrite
    if (existing) {
      fs.writeFileSync(`${file}.bak`, JSON.stringify(existing));
    }

    const merged = existing
      ? mergeContextFiles(existing, newContent)
      : newContent;

    fs.writeFileSync(file, formatContext(merged));
  } finally {
    await release();
  }
}
```

### Intra-Session (Skills)

Skills handle context within a session (no hooks fire):

**/task switch** - Save current, load new:

```
User: /task switch sdp-456

Claude (TaskContext skill):
1. Summarizes current work: "You were debugging auth token refresh..."
2. Writes to ~/.claude/history/tasks/sdp-123.md
3. Reads ~/.claude/history/tasks/sdp-456.md
4. Injects new context: "Resuming SDP-456: You were implementing..."
5. Updates CURRENT_TASK=sdp-456
```

**/park** - Force save without ending session:

```
User: /park "investigating rate limiting issue"

Claude (TaskContext skill):
1. Saves current context immediately (same as SessionEnd would)
2. Adds user note to context file
3. Confirms: "Context saved. You can safely switch or close."
```

**/spec pause** - Save spec state:

```
User: /spec pause

Claude (SpecAssist skill):
1. Saves current spec progress
2. Notes phase (specify/plan/breakdown/implement)
3. Records pending decisions
4. Writes to .specs/{spec-id}.paused.md
```

### Context File Format

**Workspace Context** (`~/.claude/history/workspaces/work:tasks.md`):

```markdown
---
workspace: work:tasks
updated: 2026-01-29T14:32:00Z
active_task: sdp-123
---

## Last Session Summary

Working on SDP-123 (auth token refresh bug). Found root cause in
TokenManager.refresh() - wasn't handling 401 responses correctly.

## Active Task

- **ID:** sdp-123
- **Status:** in-progress
- **Last action:** Reading TokenManager.ts

## Pending Questions

- Should we retry on 401 or immediately refresh?
- Need to check if refresh token is also expired

## Open Files

- src/auth/TokenManager.ts:142
- tests/auth/TokenManager.test.ts
```

**Task Context** (`~/.claude/history/tasks/sdp-123.md`):

```markdown
---
task_id: sdp-123
source: sdp
title: Auth token refresh failing intermittently
updated: 2026-01-29T14:32:00Z
spec_id: fix-auth-refresh
---

## Summary

Investigating intermittent auth failures. Users report being logged out
randomly. Found that TokenManager.refresh() doesn't handle 401 during
refresh attempt.

## Progress

1. âœ“ Reproduced issue locally
2. âœ“ Found root cause in TokenManager.ts:142
3. â†’ Implementing fix
4. â—‹ Write tests
5. â—‹ Verify fix

## Key Findings

- 401 during refresh causes infinite loop
- refresh_token might also be expired
- Need to check expiry BEFORE attempting refresh

## Next Steps

Implement retry logic with exponential backoff, add refresh token
expiry check.
```

### Complete Context Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Session Boundaries (Hooks)                                     â”‚
â”‚                                                                 â”‚
â”‚  SessionStart â”€â”€â–º Load workspace + task context                 â”‚
â”‚  SessionEnd   â”€â”€â–º Save workspace + task context                 â”‚
â”‚  PreCompact   â”€â”€â–º Save before Claude summarizes                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Hooks fire automatically
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Within Session (Skills)                                        â”‚
â”‚                                                                 â”‚
â”‚  /task switch â”€â”€â–º Save current task, load new task              â”‚
â”‚  /task pause  â”€â”€â–º Save task context (no load)                   â”‚
â”‚  /park        â”€â”€â–º Save all context immediately                  â”‚
â”‚  /spec pause  â”€â”€â–º Save spec state                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Both read/write same files
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Storage                                                        â”‚
â”‚                                                                 â”‚
â”‚  ~/.claude/history/workspaces/{workspace}.md                    â”‚
â”‚  ~/.claude/history/tasks/{task-id}.md                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Context Never Lost

| Scenario | What Saves Context |
|----------|-------------------|
| Normal session end | SessionEnd hook |
| Task switch mid-session | TaskContext skill |
| `/park` command | TaskContext skill |
| Context window fills | PreCompact hook |
| SSH disconnect | SessionEnd hook (if clean) |
| Claude Code crash | PreCompact hook (last save) + JSONL transcript |
| Instance reboot | JSONL transcript + `/resume` |

### Recovery Commands

```bash
/resume              # PAI built-in: reload from JSONL transcript
/context show        # Show current loaded context
/context reload      # Force reload from context files
/task history        # Show task context history
```

### Git Automation Controls

```bash
/git pause           # Pause auto-commit (messy refactor)
/git resume          # Resume auto-commit
/git status          # Show auto-commit state
```

### Spec Lifecycle Controls

```bash
/spec pause          # Pause spec, keep WIP branch
/spec resume         # Resume paused spec
/spec abandon        # Archive spec, delete WIP branch
/spec list --all     # Show active, paused, abandoned
```

### Parallel Tasks (Pane-Based)

Work on multiple tasks simultaneously using tmux panes with separate Claude sessions.

**Use case:** Claude is generating code or researching in one session; you want to continue working on something else without waiting.

**Layout:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ work:tasks                                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Claude session 1 (sdp-123)                                      â”‚
â”‚ [generating code...]                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Claude session 2 (sdp-456)                                      â”‚
â”‚ [interactive - you're working here]                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Commands:**

```bash
/task split              # Picker: select from actionable datahub items
/task split sdp-456      # Split with specific datahub item
/task split -v sdp-456   # Split vertically (left/right)
/task split --fresh      # Prompt for description â†’ create in datahub â†’ sync to SDP
/task focus              # Toggle focus between panes
/task close-pane         # Close current pane, save its context
/task merge              # Close secondary pane, bring context back to main
```

**Task picker (shown on `/task split` with no args):**

```
Select task for new pane:

  Actionable (work:tasks):
  > sdp-123  Auth token refresh failing       [in-progress]
    sdp-456  Add rate limiting to API         [open]
    sdp-789  Update user dashboard            [open]
    devops-12 Fix CI pipeline timeout         [open]

  [Enter] Select  [/] Filter  [n] New (--fresh)  [q] Cancel
```

**Fresh task flow (`/task split --fresh`):**

```
/task split --fresh
    â†“
Claude: "What are you working on?"
    â†“
User: "Investigating memory leak in auth service"
    â†“
Creates datahub item:
  - source: local
  - type: standalone-task
  - title: "Investigating memory leak in auth service"
  - zone: work (from current workspace)
    â†“
Queues sync to SDP (creates standalone task)
    â†“
Returns new task ID (e.g., sdp-790)
    â†“
Splits pane with new task
```

**Benefits of --fresh flow:**

| Aspect | Value |
|--------|-------|
| Nothing untracked | Even ad-hoc work gets captured |
| Single source of truth | All tasks in datahub, all synced to SDP |
| Findable later | "What was that thing I was working on?" |
| Visible to team | Standalone tasks appear in SDP |

**What happens on `/task split`:**

1. tmux splits current pane (horizontal by default)
2. New pane starts fresh Claude Code session
3. Sets `CURRENT_TASK=sdp-456` in new pane
4. Loads sdp-456 context via SessionStart hook
5. Both panes work independently

**Each pane has:**

| Property | Behavior |
|----------|----------|
| Claude session | Separate (independent context) |
| CURRENT_TASK | Different per pane |
| Context files | Saved independently |
| Git auto-commit | Both active (same repo, same WIP branch is fine) |

**Native tmux also works:**

```bash
Ctrl-b "         # Split horizontally
Ctrl-b %         # Split vertically
Ctrl-b o         # Switch between panes
Ctrl-b z         # Zoom current pane (full screen)
Ctrl-b arrows    # Resize panes
```

**When to use:**

| Situation | Approach |
|-----------|----------|
| Quick detour, will return | `/task switch` (same session) |
| Need to wait for Claude | `/task split` (parallel sessions) |
| Two related tasks simultaneously | `/task split` (parallel sessions) |
| Compare two approaches | `/task split -v` (side by side) |

### Mode Data Sources

All modes get PAI context preservation. The difference is whether items sync to external systems via datahub.

| Mode | Datahub (external sync) | PAI Context | Picker Source |
|------|-------------------------|-------------|---------------|
| **tasks** | Yes â†’ SDP (Request/Incident), DevOps | Yes | Datahub actionable items |
| **comms** | Yes â†’ Email, Slack | Yes | Datahub messages/threads |
| **projects** | Yes â†’ SDP (if owner) | Yes | Datahub owned projects |
| **research** | No | Yes | PAI history (local topics) |
| **adhoc** | Yes â†’ SDP (General Task) | Yes | Datahub + PAI history |

### Projects Mode & Ownership

Projects mode surfaces long-running initiatives you own (vs tasks assigned to you).

**Ownership defined by:**

| Source | Owner Field | You Own If |
|--------|-------------|------------|
| SDP Project | `owner` or `project_manager` | Your user ID matches |
| DevOps Project | `project.owner` | Your user ID matches |
| Local project | `owner` in frontmatter | Set to `self` |

**What appears in projects:**

```
/work projects
    â†“
"Your projects (work):
 - Imladris 2.0 (local) - 3 open tasks
 - Q1 Security Audit (SDP) - 12 open tasks
 - API Migration (DevOps) - 8 open tasks

 Select project to see tasks and progress"
```

**Project vs Task distinction:**

| Aspect | Task | Project |
|--------|------|---------|
| Duration | Hours to days | Weeks to months |
| Subtasks | No | Yes (tasks belong to projects) |
| Ownership | Assigned to you | You're responsible for outcome |
| Completion | Single action | Multiple milestones |

**Commands:**

```bash
/projects                    # List owned projects
/projects show <id>          # Project details + task breakdown
/projects status <id>        # Update project status
/projects add-task <id>      # Create task under project
```

### Comms Mode Workflow

Comms mode is for **batch processing** communications - not staying in it all day. Enter, process actionable items, exit.

**What appears in comms:**

| Source | Item Type | Triage Filter |
|--------|-----------|---------------|
| MS365/Gmail | Email | `triage: act` (needs reply/action) |
| Slack | Thread/DM | `triage: act` (needs response) |
| Telegram | Message | `triage: act` (needs response) |

**Workflow:**

```
/work comms
    â†“
Claude shows inbox summary:

"Comms inbox (work):
 ğŸ“§ Email: 4 actionable
 ğŸ’¬ Slack: 2 threads need response

 Oldest first:
 1. [email] ms365-AAMk... - Q3 budget review request (2 days ago)
 2. [email] ms365-BBNk... - Architecture decision needed (1 day ago)
 3. [slack] slack-1706... - @you: thoughts on API design? (4 hours ago)
 4. [email] ms365-CCPk... - Meeting follow-up (3 hours ago)
 ...

 Process items, or /inbox to see full list"
```

**Processing an email:**

```
User: "Let's handle the Q3 budget email"
    â†“
Claude loads full item:
- Shows email body
- Shows thread history (if reply)
- Shows sender context (from previous interactions)
    â†“
User: "Draft a reply saying I'll review by Friday"
    â†“
Claude drafts reply (Comms skill)
    â†“
User: "Send it" or "Revise: more formal"
    â†“
/comms send                    â† Queues reply to MS365
/comms done ms365-AAMk...      â† Marks item processed
```

**Processing a Slack thread:**

```
User: "What's the API design question?"
    â†“
Claude loads thread:
- Shows full thread context
- Shows who's involved
- Shows channel context
    â†“
User: "Reply that I prefer REST over GraphQL for this use case, with reasons"
    â†“
Claude drafts reply
    â†“
/slack reply slack-1706... "I'd recommend REST here because..."
    â†“
/comms done slack-1706...
```

**Comms Commands:**

```bash
# View
/inbox                         # Show all actionable comms
/inbox --email                 # Email only
/inbox --slack                 # Slack only
/inbox --unread                # Unread only

# Process
/comms show <id>               # Load full item with context
/comms draft                   # Draft reply to current item
/comms send                    # Queue send for current draft
/comms done <id>               # Mark processed (triage â†’ keep)
/comms snooze <id> <duration>  # Snooze for later (1h, tomorrow, etc.)
/comms delegate <id> <person>  # Forward/assign to someone

# Bulk
/comms archive-read            # Archive all read emails (triage â†’ keep)
/comms process-newsletters     # Auto-process newsletter pattern
```

**Key Principle: Batch, Don't Live Here**

| Pattern | Recommendation |
|---------|----------------|
| Check email constantly | âŒ Don't - use tasks mode |
| Process comms 2-3x/day | âœ… Batch processing |
| Reply immediately to everything | âŒ Urgent goes to tasks |
| Clear inbox to zero | âœ… Goal of each comms session |

**Triage Integration:**

Most emails are auto-triaged to `keep` (reference) not `act`. Only items that genuinely need your response appear in comms inbox.

| Triage Result | Examples | Where It Goes |
|---------------|----------|---------------|
| `act` | Direct questions, requests, approvals | Comms inbox |
| `keep` | CC'd emails, FYIs, receipts | Archive (searchable) |
| `delete` | Spam, newsletters (if unwanted) | Trash |

**When Comms Becomes a Task:**

If an email requires significant work (not just a reply):

```
User: "This budget review is actually a big task"
    â†“
/comms promote ms365-AAMk... --to-task
    â†“
Creates datahub task item linked to email
Syncs to SDP as Request
    â†“
"Created task sdp-890: Q3 budget review. Switch to tasks mode?"
```

### Research Mode Context

Research topics are tracked locally via PAI history, not synced externally.

**Storage:**

```
~/.claude/history/
â”œâ”€â”€ workspaces/
â”‚   â”œâ”€â”€ work:research.md      â† Current research summary
â”‚   â””â”€â”€ home:research.md
â””â”€â”€ research/                  â† Individual research topics
    â”œâ”€â”€ aws-cost-optimization.md
    â”œâ”€â”€ nix-flakes-patterns.md
    â””â”€â”€ zero-trust-architecture.md
```

**On entering `/work research`:**

```
SessionStart hook fires
    â†“
Loads ~/.claude/history/workspaces/work:research.md
    â†“
Claude shows:

"Recent research topics:
 - AWS cost optimization (last: 2 days ago)
 - Nix flakes patterns (last: 1 week ago)
 - Zero trust architecture (last: 2 weeks ago)

Continue one of these, or start new research?"
```

**Commands:**

```bash
/research                    # Enter research mode, show recent topics
/research continue <topic>   # Resume specific research
/research new "topic"        # Start new research (saved to PAI history)
/research list               # List all research topics
```

### Adhoc Mode Context

Adhoc sessions sync to SDP as **General Tasks** (standalone tasks not linked to requests/problems/changes). This makes quick work visible and findable later.

**Key difference from tasks mode:**
- **tasks** â†’ SDP Request/Incident (linked to a ticket workflow)
- **adhoc** â†’ SDP General Task (standalone, no parent entity)

**Storage:**

```
~/.claude/history/
â”œâ”€â”€ workspaces/
â”‚   â”œâ”€â”€ work:adhoc.md         â† Current adhoc summary
â”‚   â””â”€â”€ home:adhoc.md
â””â”€â”€ adhoc/                     â† Individual adhoc sessions (timestamped)
    â”œâ”€â”€ 2026-01-29-14-32.md
    â”œâ”€â”€ 2026-01-29-16-45.md
    â””â”€â”€ 2026-01-28-09-15.md

~/work/datahub/items/
â”œâ”€â”€ adhoc-2026-01-29-14-32.md  â† Synced to SDP as General Task
â””â”€â”€ ...
```

**On entering `/work adhoc`:**

```
SessionStart hook fires
    â†“
Loads ~/.claude/history/workspaces/work:adhoc.md
    â†“
Claude shows:

"Recent adhoc sessions:
 - 14:32 - Debugging why npm install hangs (SDP: adhoc-14)
 - Yesterday - Testing new tmux config (SDP: adhoc-13)
 - 2 days ago - Quick AWS CLI lookup (SDP: adhoc-12)

Continue recent session or start fresh?"
```

**Commands:**

```bash
/adhoc                    # Enter adhoc, show recent sessions
/adhoc continue           # Resume most recent
/adhoc new "description"  # Start fresh (local draft)
/adhoc promote            # Explicitly sync to SDP
/adhoc close "notes"      # Close (syncs if promoted, archives if draft)
```

**Draft/Promote Lifecycle:**

To prevent spamming SDP with micro-tasks ("testing grep arguments"), adhoc sessions start local-only and only sync when significant.

```
/adhoc new "testing grep arguments"
    â†“
Creates local-only item (type: adhoc-draft)
    â†“
Work proceeds...
    â†“
Promotion triggers (any of):
  - Session active > 15 minutes
  - User logs time: /task log 30m
  - User adds note: /task note "found the issue"
  - User explicitly: /adhoc promote
    â†“
Item updates to type: adhoc-task
    â†“
Syncs to SDP as General Task
```

**Promotion triggers:**

| Trigger | Condition | Why |
|---------|-----------|-----|
| Time threshold | Session > 15 min active | Significant investment |
| Logged time | `/task log` used | Worth tracking formally |
| Notes added | `/task note` used | Has substance |
| Explicit | `/adhoc promote` | User decides it's significant |

**Cleanup (non-promoted drafts):**

| Outcome | Behavior |
|---------|----------|
| Session ends < 15 min, no notes/time | Auto-archive locally, never syncs to SDP |
| Archived drafts | Kept 30 days locally, then purged |
| Can still search | `dh ls --type adhoc-draft` finds archived drafts |

**SDP General Task sync (promoted items only):**

| Direction | Behavior |
|-----------|----------|
| Create | Promotion â†’ queues General Task creation in SDP |
| Update | `/task note` â†’ syncs note to SDP task |
| Complete | `/adhoc close` â†’ marks SDP task complete |
| Worklogs | `/task log` â†’ syncs time to SDP task |

**Why this is better:**
- Quick experiments don't pollute professional work log
- Significant work still gets tracked
- User controls when work becomes "official"
- Findable later via local archive even if not promoted

### PAI Update Safety

Extensions are additive bolt-ons to PAI, not modifications. Updates are validated before applying.

**Extension Method:**

| Extension | Method | Modifies PAI Core? |
|-----------|--------|-------------------|
| Custom hooks | Files in `~/.claude/hooks/` | No - additive |
| Custom skills | Files in `~/.claude/skills/` | No - additive |
| Environment variables | Set by workspace commands | No - PAI reads them |
| History storage | Write to `~/.claude/history/` | No - uses existing structure |

**Potential Breaking Changes:**

| If PAI changes... | Risk |
|-------------------|------|
| Hook API (HookInput/HookOutput) | Hooks fail to execute |
| History directory structure | Context files not found |
| Skill loading mechanism | Skills not loaded |
| Settings.json schema | Hook registration fails |

**Update Validation (before applying PAI updates):**

```
1. Check PAI changelog for breaking changes
2. Run interface compatibility tests:
   - Hook API: SessionStart, SessionEnd, PreCompact still work
   - History structure: workspaces/, tasks/, research/, adhoc/ exist
   - Skill loading: markdown files in ~/.claude/skills/ load
3. If incompatible â†’ hold update, notify for manual review
4. If compatible â†’ apply, validate, rollback on failure
```

**Graceful Degradation:**

Extensions fail safely - Claude still works, just without enhancements:

| Failure | Behavior |
|---------|----------|
| Missing context file | Start fresh (warn, don't crash) |
| Hook error | Continue session (log error) |
| Skill not loaded | Claude works without skill (degraded) |
| History write fails | Warn, session continues |

---

## Appendix F: Edge Cases & Operational Concerns

### Terminology Clarification

| Term | Scope | Examples |
|------|-------|----------|
| **Item** | Any datahub entry | Email, Slack message, SDP ticket, calendar event |
| **Task** | Actionable work item | SDP Request, DevOps work item, General Task |
| **Comm** | Communication needing response | Email needing reply, Slack thread |

**Rule:** "Item" is the generic container. "Task" and "Comm" are item types filtered by mode.

### Default Mode Rationale

`/work` defaults to `comms`, not `tasks`. Why?

| Reason | Explanation |
|--------|-------------|
| Triage first | Check what came in before diving into deep work |
| Quick wins | Process easy comms before committing to tasks |
| Context loading | Comms are lighter context than task work |
| Natural flow | Most people check messages before starting work |

**Override:** `/work tasks` to go directly to tasks.

### When to Use What

| Situation | Use This | Not This |
|-----------|----------|----------|
| Quick question from existing ticket | `/task split sdp-456` | `/adhoc new` |
| Unplanned investigation | `/adhoc new "description"` | `/task split --fresh` |
| Email becomes significant work | `/comms promote` | `/adhoc new` |
| Parallel work while Claude runs | `/task split` | New terminal |

### Concurrent Write Handling

Two panes writing to same context file:

```
Pane 1: saves sdp-123.md at 14:32:01
Pane 2: saves sdp-123.md at 14:32:02
```

**Resolution:** Last-write-wins with merge attempt.

| Scenario | Handling |
|----------|----------|
| Different sections modified | Auto-merge (git-style) |
| Same section modified | Last write wins, previous in `.backup` |
| Conflicting status changes | Alert user, require manual resolution |

**Implementation:** File locks with 100ms timeout. If lock fails, queue write for retry.

### Email & Slack Threading

**Email:**

| Unit | Behavior |
|------|----------|
| Conversation thread | Single item (grouped by `conversationId`) |
| Each message | Appended to thread item as notes |
| New thread | New item |

**Slack:**

| Unit | Behavior |
|------|----------|
| Thread | Single item (grouped by `thread_ts`) |
| Channel message (no thread) | Single item |
| DM | Single item per conversation |

**Why threads, not messages:** Triage and response happen at conversation level, not per-message.

### Git Conflicts in Parallel Panes

Two panes, same repo, both auto-committing:

| Scenario | Handling |
|----------|----------|
| Different files | No conflict, both commit |
| Same file, different lines | Auto-merge on commit |
| Same file, same lines | gitwatch pauses, alerts user |

**Resolution flow:**

```
gitwatch detects conflict
    â†“
Pauses auto-commit for that repo
    â†“
Alerts: "Conflict in src/auth.ts - resolve manually"
    â†“
User resolves (or one pane finishes)
    â†“
gitwatch resumes
```

### Queue & Sync Edge Cases

| Edge Case | Handling |
|-----------|----------|
| Item deleted while write pending | Cancel queued write, log warning |
| External item deleted (404) | Move local to trash, prompt user |
| LUKS not mounted when poller runs | Poller skips cycle, logs warning |
| Initial sync with existing files | Merge by timestamp (newer wins), backup conflicts |
| Duplicate external ID | Append hash suffix, alert user |

### Tag Normalization

All tags normalized to lowercase with special chars stripped:

| Input | Normalized |
|-------|------------|
| `Urgent` | `urgent` |
| `URGENT` | `urgent` |
| `High Priority` | `high-priority` |
| `@important` | `important` |

**Sync behavior:** Normalized locally, original case preserved in external system.

### Timezone Handling

| Timestamp | Format | Timezone |
|-----------|--------|----------|
| Item `created`/`updated` | ISO 8601 | UTC |
| Display to user | Local time | User's TZ (from system) |
| Sync to external | Converted | API's expected TZ |
| Context file timestamps | ISO 8601 | UTC |

**Rule:** Store UTC, display local, convert on sync.

### Rate Limiting

| Service | Limit | Handling |
|---------|-------|----------|
| MS365 Graph | 10,000/10min | Exponential backoff, resume |
| Gmail | 250 quota units/sec | Batch requests, backoff |
| SDP | 50/min | Queue with delay |
| Slack (slackdump) | Session-based | Re-auth if blocked |
| DevOps | 200/min | Backoff |

**On rate limit:**

```
1. Log warning
2. Exponential backoff (1s, 2s, 4s, 8s, max 5min)
3. Resume from last sync point
4. Alert if >3 consecutive failures
```

### Disk Space Management

| Directory | Growth | Management |
|-----------|--------|------------|
| `/data/*/datahub/items/` | ~10MB/month | Purge trash after 365 days |
| `/data/*/datahub/queue/completed/` | Unbounded | Purge after 7 days |
| `/data/claude/` | ~100MB/month | Compact sessions older than 30 days |
| `/data/repos/` | Varies | User manages |

**Alerts:**

| Threshold | Action |
|-----------|--------|
| 80% disk | Warning in status dashboard |
| 90% disk | Alert + auto-purge old queue/trash |
| 95% disk | Pause pollers, urgent alert |

### slackdump Re-authentication

Browser session expires approximately every 30 days.

**Detection:**

```
slackdump returns auth error
    â†“
Windmill marks f/slack/sync as failed
    â†“
Status bar shows âš  slack auth (via Windmill webhook)
    â†“
Slack poller pauses (others continue)
```

**Recovery:**

```bash
# Re-run slackdump auth in browser
slackdump auth
# Update Windmill variable with new session
# Or wait for next f/ops/bws-sync.ts if stored in BWS
# Check Windmill UI â†’ Resources for green status
```

**Proactive:** Windmill script checks session age, warns at 25 days.

### Search Across Zones

| Command | Behavior |
|---------|----------|
| `/search "query"` | Current zone only |
| `/search "query" --all` | Both zones |
| `/search "query" --zone home` | Explicit zone |

**Why default to current zone:** Prevents accidental mixing of work/home context.

### Calendar Event Creation

```bash
/calendar add --work "Q3 Planning" --date 2026-02-15 --time 14:00 --duration 1h
```

| Field | Required | Default |
|-------|----------|---------|
| Title | Yes | - |
| Date | No | Today |
| Time | No | Next hour |
| Duration | No | 30min |
| Zone | No | Current zone |

**Sync:** Queued to appropriate calendar (MS365 for work, Google for home).

### Backup & Restore

**Backup layers:**

| Layer | Frequency | Retention | Restore Time |
|-------|-----------|-----------|--------------|
| EBS snapshots (DLM) | Hourly | 24 hours | 10-15 min |
| S3 sync | Daily | 90 days | 30-60 min |
| LUKS full image | Weekly | 4 weeks | 1-2 hours |

**Restore procedure:**

```bash
# From EBS snapshot (fastest)
1. Stop instance
2. Detach current data volume
3. Create volume from snapshot
4. Attach as /dev/xvdf
5. Start instance
6. imladris-unlock

# From S3 (if EBS unavailable)
1. Create fresh data volume
2. Attach and format
3. aws s3 sync s3://backup-bucket/latest /data
4. imladris-unlock

# Full LUKS image (disaster recovery)
1. Download image from S3 Glacier (may take hours)
2. dd to new EBS volume
3. Attach and unlock
```

### Error Notification

Beyond status bar, critical errors notify via:

| Severity | Channel |
|----------|---------|
| Warning | Status bar only |
| Error | Status bar + SimpleX message |
| Critical | Status bar + SimpleX + Telegram |

**What's critical:**

| Event | Severity |
|-------|----------|
| Auth expired | Warning |
| Sync failed 3x | Error |
| Disk 90%+ | Error |
| LUKS mount failed | Critical |
| All pollers down | Critical |

---

## Appendix G: Security Hardening

### Token & Credential Storage

| Credential | Location | Protection |
|------------|----------|------------|
| BWS access token | `/data/config/bws/token` | LUKS + file permissions (600) |
| OAuth refresh tokens | Windmill PostgreSQL | LUKS (DB on /data) + Windmill encryption |
| slackdump session | `/data/config/slackdump/` | LUKS + file permissions |
| SSH keys | `/data/ssh/` | LUKS + file permissions (600) |
| AWS credentials | None (instance profile) | IAM only |

**Token encryption:**

```bash
# Tokens encrypted with age before writing
age -e -R ~/.age/recipients.txt < tokens.json > tokens.json.age

# Decrypted to memory on read
age -d -i ~/.age/identity.txt < tokens.json.age
```

### Cross-Account Blast Radius

Instance compromise = access to all AWS accounts in registry.

**Mitigations:**

| Control | Implementation |
|---------|----------------|
| Least privilege | ReadOnlyAccess default, Admin requires justification |
| Session duration | 1 hour max for assumed roles |
| CloudTrail | All API calls logged in each account |
| Alerting | GuardDuty alerts on unusual patterns |
| Rotation | Instance role rotated on reboot |

**Admin access flow:**

```bash
# Via Windmill API (skill calls this)
curl -s http://localhost:8000/api/w/main/jobs/run_wait_result/f/aws/get-session \
  -d '{"account": "org-prod", "role": "Admin", "reason": "Deploying hotfix for auth bug"}'
# Reason required for Admin role
# Logged to audit trail
# Session limited to 1 hour
```

### Audit Logging

All sensitive operations logged to `/data/logs/audit.jsonl`:

```json
{
  "timestamp": "2026-01-29T14:32:00Z",
  "action": "aws_assume_role",
  "account": "org-prod",
  "role": "AdminAccess",
  "reason": "Deploying hotfix",
  "session_id": "abc123",
  "source_ip": "100.x.x.x"
}
```

**What's logged:**

| Action | Logged |
|--------|--------|
| AWS role assumption | Always |
| Admin role usage | Always + reason |
| Auth token refresh | Always |
| External API writes | Always |
| File access outside datahub | Never (too noisy) |

**Retention:** 90 days locally, synced to S3 for long-term.

### Network Security

| Component | Exposure |
|-----------|----------|
| SSH | Tailscale only (no public) |
| Pollers | Outbound HTTPS only |
| Claude/Bedrock | Outbound to AWS endpoints |
| Status TUI | localhost only |

**Firewall rules (iptables):**

```
# Allow Tailscale
-A INPUT -i tailscale0 -j ACCEPT

# Allow established connections
-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT

# Drop everything else inbound
-A INPUT -j DROP

# Allow all outbound
-A OUTPUT -j ACCEPT
```

---

## Appendix H: Windmill Orchestration

### H.1 Overview

[Windmill](https://www.windmill.dev/) is an open-source workflow engine that could replace the custom systemd timers + queue processor architecture. This appendix explores the benefits and trade-offs.

**Why consider Windmill:**
- Many more data sources planned
- Each source currently needs: systemd timer + script + queue handler + retry logic
- Windmill provides all of this out of the box

### H.2 Current vs Windmill Architecture

**Current:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ systemd timers                                              â”‚
â”‚   â”œâ”€â”€ sdp-poller.timer (5min)                              â”‚
â”‚   â”œâ”€â”€ ms365-poller.timer (5min)                            â”‚
â”‚   â”œâ”€â”€ slack-poller.timer (60s)                             â”‚
â”‚   â””â”€â”€ ... (one per source)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Bun scripts (custom)                                        â”‚
â”‚   â”œâ”€â”€ lib/pollers/sdp.ts                                   â”‚
â”‚   â”œâ”€â”€ lib/pollers/ms365.ts                                 â”‚
â”‚   â””â”€â”€ lib/queue/processor.ts  â† custom retry/backoff       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Datahub (flat files + SQLite)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**With Windmill:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Windmill (native via Nix/systemd)                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ Server (Rust) + Workers + Postgres                   â”‚  â”‚
â”‚   â”‚                                                       â”‚  â”‚
â”‚   â”‚ Schedules:           Scripts:                        â”‚  â”‚
â”‚   â”‚   sdp: */5 * * * *     f/sdp/sync.ts                â”‚  â”‚
â”‚   â”‚   ms365: */5 * * * *   f/ms365/sync.ts              â”‚  â”‚
â”‚   â”‚   slack: * * * * *     f/slack/sync.ts              â”‚  â”‚
â”‚   â”‚                                                       â”‚  â”‚
â”‚   â”‚ Built-in: retries, backoff, logging, UI             â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Datahub (flat files + SQLite)  [unchanged]                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### H.3 Feature Comparison

| Feature | Current (systemd) | Windmill |
|---------|-------------------|----------|
| Scheduling | systemd timers | Built-in cron |
| Retries | Custom per-script | Built-in with policies |
| Backoff | Custom per-script | Configurable |
| Monitoring | journalctl | Web UI dashboard |
| Adding new source | timer + script + handler | Just add script |
| Error visibility | Grep logs | UI with alerts |
| Webhooks | Custom endpoint | Built-in |
| Flows | Manual chaining | Visual composer |
| Approval gates | N/A | Built-in |
| Language support | Bun only | Bun, Python, Go, Bash |

### H.4 Deployment

**Native Deployment (Nix + systemd):**

No Docker required. Windmill is a Rust binary that runs natively.

```nix
# nix/home.nix or system configuration

# Postgres for Windmill job queue
services.postgresql = {
  enable = true;
  ensureDatabases = [ "windmill" ];
  ensureUsers = [{
    name = "windmill";
    ensureDBOwnership = true;
  }];
};

# Windmill server
systemd.services.windmill-server = {
  description = "Windmill Server";
  after = [ "postgresql.service" ];
  wantedBy = [ "multi-user.target" ];
  environment = {
    DATABASE_URL = "postgres://windmill@localhost/windmill";
    BASE_URL = "http://localhost:8000";
  };
  serviceConfig = {
    ExecStart = "${pkgs.windmill}/bin/windmill --mode server";
    Restart = "always";
    User = "windmill";
  };
};

# Windmill worker
systemd.services.windmill-worker = {
  description = "Windmill Worker";
  after = [ "windmill-server.service" ];
  wantedBy = [ "multi-user.target" ];
  environment = {
    DATABASE_URL = "postgres://windmill@localhost/windmill";
  };
  serviceConfig = {
    ExecStart = "${pkgs.windmill}/bin/windmill --mode worker";
    EnvironmentFile = "/data/.env";  # Contains BWS_ACCESS_TOKEN
    Restart = "always";
    User = "windmill";
  };
};
```

**Resource estimate (native, no container overhead):**
- Postgres: ~50MB RAM
- Windmill server: ~100MB RAM
- Windmill worker: ~50MB RAM + script overhead
- Total: ~200MB baseline

**Windmill UI Access:**

| Access From | URL | Auth |
|-------------|-----|------|
| Tailscale network | `http://imladris:8000` | None (trusted network) |
| SSH tunnel | `ssh -L 8000:localhost:8000 imladris` | SSH key |

**Rationale:** Tailscale provides authentication at the network layer. If you're on the Tailscale network, you've already authenticated. No additional Windmill auth required.

**Nix configuration:**

```nix
# Bind to Tailscale interface (not just localhost)
systemd.services.windmill-server.environment = {
  BASE_URL = "http://imladris:8000";
  LISTEN_ADDR = "0.0.0.0:8000";  # Tailscale interface
};

# Firewall: only allow from Tailscale
networking.firewall.interfaces."tailscale0".allowedTCPPorts = [ 8000 ];
```

### H.5 Script Language Guidelines

**Default: TypeScript (Bun)**

Aligns with PAI, fast startup, consistent codebase.

**Exception: Python when library is significantly better**

| Use Case | Language | Reason |
|----------|----------|--------|
| HTTP API integrations | TypeScript | Simple fetch, consistent |
| Datahub read/write | TypeScript | Matches PAI stack |
| Ops scripts (backup, cleanup) | TypeScript | Consistency |
| AWS complex operations | Python | boto3 maturity |
| Data processing (if needed) | Python | pandas/numpy |
| ML/AI operations | Python | Library ecosystem |

**Script templates:**

```typescript
// TypeScript (default) - f/sdp/sync.ts
import * as wmill from "windmill-client";

export async function main() {
    const token = await wmill.getVariable("sdp_api_token");
    const resp = await fetch("https://sdp.example.com/api/v3/requests", {
        headers: { "Authorization": `Bearer ${token}` }
    });
    const data = await resp.json();
    await writeToDatahub(data.requests);
    return { synced: data.requests.length };
}
```

```python
# Python (when needed) - f/aws/org-accounts.py
import wmill
import boto3

def main():
    session = boto3.Session()
    org = session.client("organizations")
    accounts = org.list_accounts()["Accounts"]
    return {"accounts": accounts}
```

**Rule of thumb:** Start with TypeScript. Switch to Python only if you're fighting the language or missing critical libraries.

### H.6 Windmill Best Practices

#### Folder Organization

Use folder-based paths with clear ownership:

```
f/                          # Shared folders
â”œâ”€â”€ sdp/                    # ServiceDesk Plus
â”‚   â”œâ”€â”€ sync.ts
â”‚   â”œâ”€â”€ get-tickets.ts
â”‚   â””â”€â”€ update-ticket.ts
â”œâ”€â”€ ms365/                  # Microsoft 365
â”œâ”€â”€ aws/                    # AWS operations
â”œâ”€â”€ ops/                    # Backups, maintenance
â””â”€â”€ triage/                 # AI processing

u/                          # User-specific (if needed)
â””â”€â”€ admin/
    â””â”€â”€ debug-scripts/
```

#### Variables vs Resources vs Secrets

| Type | Use For | Example |
|------|---------|---------|
| **Variable** | Simple config strings | `sdp-base-url`, `default-zone` |
| **Secret** | API keys, tokens (encrypted) | `sdp-api-token`, `telegram-bot-token` |
| **Resource** | Structured + OAuth (auto-refresh) | `ms365-oauth`, `slack-oauth`, `postgres-db` |

```typescript
// Variable (simple string)
const baseUrl = await wmill.getVariable("f/sdp/base-url");

// Secret (encrypted, audit-logged)
const token = await wmill.getVariable("f/sdp/api-token");  // same API, marked secret in UI

// Resource (structured, OAuth auto-refresh)
const ms365 = await wmill.getResource("f/ms365/oauth");
const accessToken = ms365.token;  // Always valid, Windmill refreshes
```

#### Error Handling

**Per-script retry policy** (in metadata YAML):

```yaml
# sync.script.yaml
retry:
  max_attempts: 3
  exponential_delay:
    base_ms: 1000
    multiplier: 2
    max_ms: 60000
```

**Workspace-level error handler** â€” notify on any failure:

```typescript
// f/ops/error-handler.ts
// Set as workspace error handler in Settings

export async function main(error: {
    job_id: string;
    path: string;
    error: string;
}) {
    await sendTelegramMessage(`âŒ Job failed: ${error.path}\n${error.error}`);
}
```

**Flow error handling:**

```yaml
# flow.yaml
error_handler:
  path: f/ops/error-handler

steps:
  - id: fetch
    script: f/sdp/sync
    retry:
      max_attempts: 3
    continue_on_error: false  # Stop flow if this fails

  - id: notify
    script: f/ops/send-summary
    continue_on_error: true   # Don't fail flow if notification fails
```

#### Git Sync

**Pull without secrets** (for version control):

```bash
wmill sync pull --skip-secrets --skip-resources
```

**Folder structure after sync:**

```
windmill/
â”œâ”€â”€ f/
â”‚   â”œâ”€â”€ sdp/
â”‚   â”‚   â”œâ”€â”€ sync.ts
â”‚   â”‚   â””â”€â”€ sync.script.yaml
â”‚   â””â”€â”€ ...
â”œâ”€â”€ wmill.yaml
â””â”€â”€ .gitignore           # Exclude secrets
```

**Git workflow:**

```
1. Edit locally (VS Code + Windmill extension)
2. Test in Windmill UI
3. wmill sync push
4. Commit to git
```

#### Naming Conventions

| Item | Pattern | Example |
|------|---------|---------|
| Scripts | `{action}.ts` or `{action}-{noun}.ts` | `sync.ts`, `get-tickets.ts` |
| Flows | `{process}-flow` | `full-sync-flow` |
| Variables | `{service}-{item}` | `sdp-base-url` |
| Secrets | `{service}-{credential}` | `sdp-api-token` |
| Resources | `{service}-{type}` | `ms365-oauth` |

#### Timeouts

Set appropriate timeouts per script:

| Script Type | Timeout |
|-------------|---------|
| Quick queries | 30s |
| Sync operations | 5m |
| Batch processing | 15m |
| Backups | 30m |

```yaml
# sync.script.yaml
timeout_s: 300  # 5 minutes
```

### H.7 Poller Migration Example

**Current (sdp-poller.ts):**

```typescript
// lib/pollers/sdp.ts
import { fetchRequests, writeToDatahub } from './common';

async function poll() {
    try {
        const requests = await fetchRequests();
        await writeToDatahub(requests);
    } catch (e) {
        // Custom retry logic
        await sleep(exponentialBackoff(retryCount));
        // ...
    }
}
```

**Windmill version:**

```typescript
// windmill script: pollers/sdp
// Schedule: */5 * * * *
// Retry policy: 3 attempts, exponential backoff

import * as wmill from "windmill-client";

export async function main() {
    const sdpToken = await wmill.getVariable("sdp_api_token");
    const requests = await fetchRequests(sdpToken);
    await writeToDatahub(requests);
    return { synced: requests.length };
}
```

Retry/backoff handled by Windmill, not custom code.

### H.8 New Capabilities

**Webhooks for real-time:**

Instead of polling Slack every 60s, receive events instantly:

```typescript
// windmill webhook: slack-events
// Triggered by: Slack Event API

export async function main(event: SlackEvent) {
    if (event.type === "message") {
        await writeToDatahub(event);
    }
}
```

**Flows for complex ingestion:**

```yaml
# windmill flow: full-sync
summary: Complete sync of all sources
steps:
  - id: sdp
    script: pollers/sdp
  - id: ms365
    script: pollers/ms365
    parallel: true
  - id: slack
    script: pollers/slack
    parallel: true
  - id: triage
    script: triage/batch
    depends_on: [sdp, ms365, slack]
```

**Approval gates for sensitive ops:**

```typescript
// windmill flow: admin-action
steps:
  - id: request
    script: prepare-admin-action
  - id: approve
    type: approval
    timeout: 1h
  - id: execute
    script: execute-admin-action
    depends_on: [approve]
```

### H.9 Windmill as Integration Gateway

**Core principle:** Claude/PAI never calls external APIs directly. All external communication routes through Windmill.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Claude / PAI                                                    â”‚
â”‚                                                                 â”‚
â”‚  "get ramp expenses"         "update SDP-1234"                 â”‚
â”‚  "check securonix alerts"    "send slack message"              â”‚
â”‚           â”‚                           â”‚                         â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                       â–¼                                         â”‚
â”‚              Windmill skill                                     â”‚
â”‚         (routes to appropriate script)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Windmill                                                        â”‚
â”‚                                                                 â”‚
â”‚  scripts/                                                       â”‚
â”‚    ramp/get-expenses.ts        (on-demand)                     â”‚
â”‚    ramp/sync.ts                (scheduled: 0 */6 * * *)        â”‚
â”‚    securonix/get-alerts.ts     (on-demand)                     â”‚
â”‚    securonix/sync.ts           (scheduled: */5 * * * *)        â”‚
â”‚    sdp/get-ticket.ts           (on-demand)                     â”‚
â”‚    sdp/update-ticket.ts        (on-demand)                     â”‚
â”‚    sdp/sync.ts                 (scheduled: */5 * * * *)        â”‚
â”‚    slack/send-message.ts       (on-demand)                     â”‚
â”‚    slack/sync.ts               (scheduled: * * * * *)          â”‚
â”‚                                                                 â”‚
â”‚  All credentials in Windmill variables (synced from BWS)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
                 External APIs
```

**Script categories:**

| Category | Trigger | Purpose |
|----------|---------|---------|
| `*/sync.ts` | Scheduled | Periodic data pull â†’ datahub |
| `*/get-*.ts` | On-demand | Adhoc queries, return data directly |
| `*/update-*.ts` | On-demand | Write-back to external system |
| `*/send-*.ts` | On-demand | Outbound messages |

**Benefits of gateway pattern:**

| Concern | Solution |
|---------|----------|
| Credentials scattered | All in Windmill variables |
| Inconsistent error handling | Windmill retry policies |
| No audit trail | Windmill job history |
| Rate limit management | Windmill concurrency controls |
| Adding new source | Just add scripts, skill routes automatically |

### H.10 Interactive vs Scheduled Access

For **scheduled syncs**, all calls go through Windmillâ€”latency doesn't matter.

For **interactive work** (AI or human doing rapid-fire queries), Windmill acts as a **credential provider** rather than a proxy:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scheduled (background sync)                                     â”‚
â”‚                                                                 â”‚
â”‚   Windmill schedule â†’ script â†’ External API â†’ datahub          â”‚
â”‚   (latency doesn't matter, full audit trail)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Interactive (AI or human)                                       â”‚
â”‚                                                                 â”‚
â”‚   Step 1: Get credentials                                       â”‚
â”‚   Claude â†’ Windmill â†’ get-creds script â†’ return token/creds    â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ once â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                                 â”‚
â”‚   Step 2: Use directly (no Windmill overhead)                  â”‚
â”‚   Claude â†’ External API (using cached creds)                   â”‚
â”‚   Claude â†’ External API                                        â”‚
â”‚   Claude â†’ External API                                        â”‚
â”‚            â””â”€â”€ rapid-fire, no 50ms overhead per call â”€â”€â”˜       â”‚
â”‚                                                                 â”‚
â”‚   Step 3: Re-fetch when expired                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Credential scripts:**

```typescript
// f/aws/get-session.ts
// Returns temporary credentials for interactive AWS work
export async function main(account: string, role: string = "ReadOnly") {
    const roleArn = `arn:aws:iam::${account}:role/Imladris${role}`;
    const creds = await sts.assumeRole({
        RoleArn: roleArn,
        RoleSessionName: "windmill-interactive",
        DurationSeconds: 3600
    });
    return {
        accessKeyId: creds.Credentials.AccessKeyId,
        secretAccessKey: creds.Credentials.SecretAccessKey,
        sessionToken: creds.Credentials.SessionToken,
        expiration: creds.Credentials.Expiration,
        region: "us-east-1"
    };
}

// scripts/ms365/get-token.ts
// Returns OAuth token for interactive MS365 work
export async function main() {
    const resource = await wmill.getResource("ms365_oauth");
    return {
        accessToken: resource.token,
        expiration: resource.expires_at
    };
}

// scripts/slack/get-token.ts
export async function main() {
    const resource = await wmill.getResource("slack_oauth");
    return { token: resource.token };
}
```

**Access patterns by service:**

| Service | Scheduled Sync | Interactive Access |
|---------|---------------|-------------------|
| AWS | Through Windmill | Get creds â†’ use directly |
| MS365 (Graph API) | Through Windmill | Get token â†’ use directly |
| Gmail/GCal | Through Windmill | Get token â†’ use directly |
| Slack | Through Windmill | Get token â†’ use directly |
| SDP | Through Windmill | Through Windmill (low volume) |
| Ramp | Through Windmill | Through Windmill (low volume) |
| Securonix | Through Windmill | Get token â†’ use directly |

**Rule of thumb:** If you might issue 10+ commands in a session, get credentials and call directly. If it's occasional queries, route through Windmill for the audit trail.

**Curu skill (PAI) handles both patterns:**

```markdown
# In Windmill skill

## Interactive session setup
- "aws session for prod" â†’ get-session(account, role) â†’ cache creds
- "connect to ms365" â†’ get-token() â†’ cache token

## Then use cached creds for direct calls
- Uses AWS SDK with cached credentials
- Uses Graph API with cached token
```

### H.11 Why Windmill

| Factor | Windmill |
|--------|----------|
| Footprint | ~150MB steady state (Rust backend) |
| Performance | 26M tasks/month on single $5 worker |
| API | Full REST API for Curu skill (PAI) integration |
| Adding sources | Script + schedule = done |
| Monitoring | Built-in web UI |
| Retries | Configurable policies, no custom code |
| Webhooks | Native support for real-time |
| Languages | Bun, Python, Go, Bash |
| Open source | AGPLv3, no vendor lock-in |

### H.12 Curu Skills (PAI): One Per Windmill Module

**Pattern:** One thin skill per Windmill script folder. Each skill routes requests to its module's scripts.

**Skill structure:**

```
Windmill scripts/          Curu Skills (PAI) (thin routing)
â”œâ”€â”€ f/sdp/            â†’    SDP skill
â”œâ”€â”€ f/ms365/          â†’    MS365 skill
â”œâ”€â”€ f/slack/          â†’    Slack skill
â”œâ”€â”€ f/aws/            â†’    AWS skill
â”œâ”€â”€ f/ramp/           â†’    Ramp skill
â”œâ”€â”€ f/securonix/      â†’    Securonix skill
â”œâ”€â”€ f/ops/            â†’    Ops skill
â””â”€â”€ f/triage/         â†’    (uses Triage skill, not routing)
```

**Example skill template:**

```markdown
# Skill: SDP

Routes SDP requests to Windmill scripts in f/sdp/.

## Scripts Available
- f/sdp/sync.ts - Scheduled sync (don't call directly)
- f/sdp/get-tickets.ts - List tickets
- f/sdp/get-ticket.ts - Get single ticket
- f/sdp/update-ticket.ts - Update ticket

## Triggers
- "sdp tickets" â†’ run f/sdp/get-tickets
- "show SDP-1234" â†’ run f/sdp/get-ticket {id: "1234"}
- "update SDP-1234 status to resolved" â†’ run f/sdp/update-ticket {id: "1234", status: "resolved"}
- "sync sdp" â†’ run f/sdp/sync

## API Pattern
POST /api/w/main/jobs/run_wait_result/p/f/sdp/get-tickets
Body: { "args": { "status": "open" } }
```

**Why skill-per-module:**

| Aspect | One Giant Skill | Skill Per Module |
|--------|-----------------|------------------|
| Size | Grows unbounded | ~20 lines each |
| Maintenance | Hard to find things | Clear ownership |
| Adding source | Edit huge file | Add new skill |
| Conflicts | Possible | Isolated |

**Skill inventory:**

| Skill | Module | Purpose |
|-------|--------|---------|
| SDP | f/sdp/ | ServiceDesk Plus tickets |
| MS365 | f/ms365/ | Email, calendar |
| Slack | f/slack/ | Messages |
| AWS | f/aws/ | Cross-account, S3, etc. |
| Ramp | f/ramp/ | Expenses |
| Securonix | f/securonix/ | Security alerts |
| Ops | f/ops/ | Backups, sync, maintenance |
| Triage | (built-in) | AI classification |

**Adding new integration:**

Use OpenAPI codegen for scaffolding, then consolidate into focused scripts.

```
Step 1: Scaffold (understand the API quickly)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
bunx @windmill-labs/openapi-codegen-cli \
  --schemaUrl "https://api.newservice.com/openapi.json" \
  --outputDir "./f/newservice/_generated" \
  --resourceTypeName "newservice" \
  --authKind "bearer"

Step 2: Add credential
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Add API key/token to BWS: work-newservice-api-key
- Run f/ops/bws-sync.ts to populate Windmill

Step 3: Consolidate (reduce AI thinking)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Write focused scripts that group operations by user intent:

f/newservice/
â”œâ”€â”€ sync.ts          # Scheduled poller
â”œâ”€â”€ query.ts         # Consolidated query interface
â”œâ”€â”€ update.ts        # Write operations (if bidirectional)
â””â”€â”€ _generated/      # Reference only, not called by skills

Step 4: Create thin skill
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Skill: NewService

- f/newservice/query.ts - All queries (list, get, search)
- f/newservice/update.ts - Modifications
- f/newservice/sync.ts - Scheduled, don't call

Step 5: Test & schedule
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Run sync.ts manually, verify datahub files created
- Add schedule in Windmill UI
- Verify skill routing works
```

**Why consolidate?** PAI principle: reduce AI decision-making.

- Bad: 50 auto-generated scripts â†’ Claude must explore, reason, choose
- Good: 3 focused scripts â†’ deterministic routing, fewer tokens

**Time estimate:** ~45 min (codegen: 5 min, consolidate: 25 min, skill: 5 min, test: 10 min)

### H.13 Implementation Plan

| Phase | Scope |
|-------|-------|
| 1 | Add Windmill to Nix config, deploy via systemd |
| 2 | Create per-module Curu skills (PAI) |
| 3 | Migrate pollers (telegram, sdp, ms365, etc.) |
| 4 | Migrate all scheduled tasks (backups, sync, cleanup) |
| 5 | Remove systemd timers |
| 6 | Add new sources (Ramp, Securonix, etc.) |
| 7 | Enable webhooks for real-time sources |

### H.14 Unified Scheduler

**Principle:** All application-level scheduled tasks run in Windmill. systemd only handles bootstrap/system services.

**Tasks moving to Windmill:**

| Task | Schedule | Script |
|------|----------|--------|
| **Data Pollers** | | |
| SDP sync | `*/5 * * * *` | `f/sdp/sync.ts` |
| MS365 mail sync | `*/5 * * * *` | `f/ms365/sync.ts` |
| Gmail sync | `*/5 * * * *` | `f/gmail/sync.ts` |
| Slack sync | `* * * * *` | `f/slack/sync.ts` |
| Telegram sync | `* * * * *` | `f/telegram/sync.ts` |
| DevOps sync | `*/5 * * * *` | `f/devops/sync.ts` |
| Calendar sync | `*/15 * * * *` | `f/calendar/sync.ts` |
| **Backups** | | |
| Stateful backup | `0 * * * *` (hourly) | `f/ops/backup-stateful.ts` |
| S3 offsite sync | `0 6 * * *` (daily 6am) | `f/ops/backup-s3.ts` |
| **Maintenance** | | |
| Log cleanup | `0 3 * * *` (daily 3am) | `f/ops/log-cleanup.ts` |
| Temp file cleanup | `0 4 * * *` (daily 4am) | `f/ops/temp-cleanup.ts` |
| Index optimization | `0 5 * * 0` (weekly Sun) | `f/ops/index-optimize.ts` |
| **Sync** | | |
| BWS â†’ Windmill vars | `*/30 * * * *` | `f/ops/bws-sync.ts` |
| Update check | `0 0 * * *` (midnight) | `f/ops/update-check.ts` |
| Session git sync | `*/5 * * * *` | `f/ops/session-sync.ts` |
| **Triage** | | |
| Batch triage | `*/15 * * * *` | `f/triage/batch.ts` |

**Stays in systemd (bootstrap/system-level):**

| Service | Reason |
|---------|--------|
| `postgresql.service` | Windmill depends on it |
| `windmill-server.service` | Core infrastructure |
| `windmill-worker.service` | Core infrastructure |
| `tailscaled.service` | Network access |
| `luks-unlock` (oneshot) | Runs before Windmill |

**Directory structure in Windmill:**

```
f/                              # Shared folder (Windmill convention)
â”œâ”€â”€ sdp/
â”‚   â”œâ”€â”€ sync.ts                 # Scheduled poller
â”‚   â”œâ”€â”€ get-tickets.ts          # On-demand query
â”‚   â””â”€â”€ update-ticket.ts        # Write-back
â”œâ”€â”€ ms365/
â”œâ”€â”€ gmail/
â”œâ”€â”€ slack/
â”œâ”€â”€ aws/
â”‚   â””â”€â”€ get-session.ts          # Interactive credentials
â”œâ”€â”€ ops/
â”‚   â”œâ”€â”€ backup-stateful.ts
â”‚   â”œâ”€â”€ backup-s3.ts
â”‚   â”œâ”€â”€ log-cleanup.ts
â”‚   â”œâ”€â”€ bws-sync.ts
â”‚   â”œâ”€â”€ update-check.ts
â”‚   â””â”€â”€ session-sync.ts
â””â”€â”€ triage/
    â””â”€â”€ batch.ts
```

### H.15 Decision

**Adopted: Windmill as unified scheduler and integration gateway.**

Rationale:
- Single pane of glass for all scheduled tasks
- Consistent monitoring, logging, retry policies
- Many data sources planned â€” Windmill makes adding each trivial
- Small footprint (~150MB) with excellent performance (Rust)
- Full API enables Curu skill (PAI) integration
- Built-in monitoring eliminates custom dashboard work
- Webhook support enables real-time where available
- Scripts remain portable (just TypeScript/Python)

---

## Appendix I: MCP Servers

### I.1 Purpose

MCP (Model Context Protocol) servers extend Claude Code's capabilities by connecting it to external tools, APIs, and data sources.

**Guiding principle:** MCP servers should provide **external data or deterministic tooling**, not duplicate PAI's reasoning capabilities.

### I.2 MCP vs PAI Boundary

| Capability | Handled By | Rationale |
|------------|------------|-----------|
| Structured reasoning | PAI Algorithm v1.4.0 | 7-phase + ISC + constraint extraction + drift prevention |
| Complex problem-solving | PAI (pai-agents-skill) | Agent packs for multi-perspective analysis |
| Security/code review | PAI (pai-redteam-skill) | 32 adversarial agents for thorough review |
| **External documentation** | **MCP (Context7)** | Real-time data, not reasoning |
| **Deterministic code scanning** | **MCP (Lucidity)** | Automated analysis, not AI judgment |

**What we avoid:** MCP servers that duplicate PAI's reasoning (e.g., Sequential Thinking, Deep Code Reasoning). These add complexity without benefit since PAI already provides structured thinking via skills and the Algorithm v1.4.0.

### I.2.1 MCP vs Windmill

Both provide external data access. Use the right tool for the job:

| Factor | MCP | Windmill |
|--------|-----|----------|
| **Auth needed** | No (public data) | Yes (OAuth, API keys) |
| **Write operations** | No | Yes |
| **Custom logic** | Limited | Full control |
| **Claude integration** | Native (direct call) | Via skill routing |
| **Setup** | One line config | Write script |
| **Pre-built** | Yes (Context7, etc.) | No (your code) |

**Decision matrix:**

| Use Case | Tool | Why |
|----------|------|-----|
| Library documentation | MCP (Context7) | Pre-built, no auth, read-only |
| Code quality scan | MCP (Lucidity) | Pre-built, deterministic |
| SDP tickets | Windmill | Auth required, write-back |
| MS365 email | Windmill | OAuth, custom transforms |
| Slack messages | Windmill | OAuth, audit trail |
| Fresh API docs | MCP | No auth, read-only |
| Internal systems | Windmill | Auth, custom logic |

**Simple rule:**
- **MCP** = Read-only, public, pre-built
- **Windmill** = Auth, write, custom

### I.3 Recommended MCP Servers

| MCP Server | Purpose | Why It's Additive |
|------------|---------|-------------------|
| **Lucidity** | Pre-commit code quality analysis | Deterministic scanning (not AI reasoning) |
| **Context7** | Real-time documentation/API references | External data source (not reasoning pattern) |

### I.4 Lucidity MCP

Deterministic code quality analysis that reviews code before commits.

**Quality dimensions analyzed:**

| Dimension | What It Checks |
|-----------|----------------|
| Complexity | Cyclomatic complexity, nesting depth |
| Security | Injection risks, credential exposure |
| Performance | N+1 queries, unnecessary loops |
| Test Coverage | Missing test cases |
| Style | Consistency, naming conventions |
| Duplication | Copy-paste code, DRY violations |
| Error Handling | Unhandled exceptions, silent failures |
| Documentation | Missing docstrings, unclear APIs |
| Dependencies | Outdated packages, security vulnerabilities |
| Maintainability | Code smells, technical debt |

**Integration with Git automation:**

```
File change detected (debounce 30s)
    â†“
Lucidity analyzes changed files
    â†“
Issues found? â†’ Block commit â†’ Show issues
    â†“
No issues â†’ Proceed with auto-commit
```

**Installation:**

```bash
claude mcp add lucidity -- npx -y lucidity-mcp
```

**Source:** [github.com/hyperb1iss/lucidity-mcp](https://github.com/hyperb1iss/lucidity-mcp)

### I.5 Context7 MCP

Provides real-time access to current documentation and API references.

**Problem it solves:**
- Claude's training data has a knowledge cutoff
- APIs change, documentation updates
- Using outdated patterns causes bugs

**What it does:**
- Fetches current version documentation on demand
- Provides accurate API signatures and examples
- Prevents "hallucinated" API calls that don't exist

**When invoked:**
- Working with external libraries/frameworks
- Writing API integrations
- Any code touching third-party dependencies

**Installation:**

```bash
claude mcp add context7 -- npx -y @upstash/context7-mcp
```

**Source:** [github.com/upstash/context7](https://github.com/upstash/context7)

### I.6 Configuration

**Config location:** `~/.claude/settings.json` (symlinked to `/data/claude/settings.json`)

**Example configuration:**

```json
{
  "mcpServers": {
    "lucidity": {
      "command": "npx",
      "args": ["-y", "lucidity-mcp"]
    },
    "context7": {
      "command": "npx",
      "args": ["-y", "@upstash/context7-mcp"]
    }
  }
}
```

**Verify installation:**

```bash
claude mcp list
```

### I.7 MCP Tool Search (Lazy Loading)

Claude Code's MCP Tool Search enables lazy loading for MCP servers, reducing context usage by up to 95%. Servers are only loaded when their capabilities are needed.

**Benefit:** Run multiple MCP servers without context overhead until they're actually invoked.

### I.8 Integration with Workflow

| Phase | Tool | Usage |
|-------|------|-------|
| `/specify` | PAI skills | Structure requirements gathering |
| `/plan` | PAI skills | Work through architectural options |
| `/breakdown` | PAI skills | Decompose into implementable units |
| `/implement` | Lucidity, Context7 | Quality checks, accurate APIs |
| `/verify` | Lucidity, pai-redteam-skill | Final quality gate before merge |

### I.9 Excluded MCP Servers

These MCP servers were evaluated and excluded to avoid duplicating PAI:

| MCP Server | Why Excluded |
|------------|--------------|
| Sequential Thinking | PAI's Algorithm v1.4.0 already provides structured reasoning |
| Deep Code Reasoning | pai-redteam-skill covers code analysis with 32 agent perspectives |
| Other "reasoning" MCPs | Single-thread Claude + PAI skills is the preferred pattern |

### I.10 Future Considerations

| MCP Server | Purpose | Status |
|------------|---------|--------|
| Chrome DevTools | Web debugging, network analysis | Evaluate |
| Playwright | Browser automation testing | Evaluate |
| GitHub | PR/issue integration | Evaluate (vs gh CLI) |
| Database | Direct SQL access | Evaluate (vs Windmill) |

---

## Appendix J: Recommended Client Setup (Kitty)

Terminal configuration for keyboard-driven workflow with inline image support. Useful for Claude Code workflows where viewing screenshots/diagrams inline improves context.

### J.1 Why Kitty

| Feature | Benefit for Claude workflows |
|---------|------------------------------|
| Inline images | View screenshots before asking Claude to analyze |
| Kitty graphics protocol | Industry standard, widely supported |
| Keyboard hints | Copy paths, URLs, hashes without mouse |
| Light resource usage | Efficient on Linux |
| Simple config | INI-style, not a programming language |

### J.2 Server-Side Config (imladris)

**Minimal tmux additions (`~/.tmux.conf`):**

```bash
# Graphics passthrough for Kitty
set -g allow-passthrough on
set -ga terminal-overrides ',xterm-kitty:Tc'
```

That's all that's needed on imladris. Everything else is client-side.

### J.3 Client-Side Config (Aurora/Fedora)

**Install Kitty:**

```bash
# Preferred: native package (full permissions)
rpm-ostree install kitty

# Alternative: Flatpak (may have sandbox issues)
flatpak install flathub com.github.kovidgoyal.kitty
```

**Create `~/.config/kitty/kitty.conf`:**

```bash
# Font
font_family      JetBrains Mono
font_size        13

# No bells
enable_audio_bell no

# Allow scripting (needed for remote control, icat)
allow_remote_control yes

# Scrollback
scrollback_lines 10000

# URLs clickable
detect_urls yes

# Keyboard hints (Quick Select equivalent)
map ctrl+shift+e open_url_with_hints
map ctrl+shift+p>f kitten hints --type path --program -
map ctrl+shift+p>h kitten hints --type hash --program -
map ctrl+shift+p>l kitten hints --type line --program -
map ctrl+shift+p>w kitten hints --type word --program -

# Copy/paste
map ctrl+shift+c copy_to_clipboard
map ctrl+shift+v paste_from_clipboard

# Font size
map ctrl+equal change_font_size all +1.0
map ctrl+minus change_font_size all -1.0
map ctrl+0 change_font_size all 0

# Window navigation (when not using tmux)
map alt+h neighboring_window left
map alt+j neighboring_window down
map alt+k neighboring_window up
map alt+l neighboring_window right

# New windows/tabs
map ctrl+shift+enter new_window_with_cwd
map ctrl+shift+t new_tab_with_cwd
```

### J.4 Image Viewing

```bash
# View image inline
kitty +kitten icat screenshot.png

# With size constraints
kitty +kitten icat --place 80x24@0x0 diagram.png

# Clear images
kitty +kitten icat --clear

# Alias for convenience (add to ~/.zshrc on client)
alias icat='kitty +kitten icat'
```

**Through SSH + tmux:**

Images display locally even when viewing remote content, thanks to `allow-passthrough`.

### J.5 Keyboard Hints (Quick Select)

Copy text from terminal output without mouse:

| Keys | What it highlights |
|------|--------------------|
| `Ctrl+Shift+E` | URLs (opens in browser) |
| `Ctrl+Shift+P` then `f` | File paths |
| `Ctrl+Shift+P` then `h` | Git hashes |
| `Ctrl+Shift+P` then `l` | Lines |
| `Ctrl+Shift+P` then `w` | Words |

**Workflow:**

1. Press key combo (e.g., `Ctrl+Shift+P, f`)
2. All matching patterns get highlighted with labels
3. Type the label to copy that text
4. Paste with `Ctrl+Shift+V`

### J.6 SSH Integration

Kitty has enhanced SSH with automatic shell integration:

```bash
# Regular SSH (works fine)
ssh imladris

# Kitty SSH (copies terminfo, enables features)
kitty +kitten ssh imladris
```

The kitten version ensures Kitty features work correctly on the remote host.

### J.7 Claude Code Workflow with Images

```bash
# 1. Take screenshot of issue
# (use Flameshot, grim, or similar)

# 2. View it locally first
icat ~/screenshots/error.png

# 3. SSH to imladris
ssh imladris

# 4. Attach to Claude session
tmux attach -t main

# 5. Ask Claude to analyze
# Claude reads the file, you see context
claude "analyze the error in ~/screenshots/error.png"

# 6. View Claude's suggested fix result
icat ~/screenshots/after.png
```

### J.8 Optional: Tmux Keyboard-Driven Config

For keyboard-driven tmux (on imladris), add to `~/.tmux.conf`:

```bash
# Prefix: Ctrl-Space (easier than Ctrl-b)
unbind C-b
set -g prefix C-Space
bind C-Space send-prefix

# No escape delay
set -sg escape-time 0

# vim-style pane navigation (no prefix needed)
bind -n M-h select-pane -L
bind -n M-j select-pane -D
bind -n M-k select-pane -U
bind -n M-l select-pane -R

# Direct window access
bind -n M-1 select-window -t 1
bind -n M-2 select-window -t 2
bind -n M-3 select-window -t 3
bind -n M-4 select-window -t 4
bind -n M-5 select-window -t 5

# vim-style splits
bind - split-window -v -c "#{pane_current_path}"
bind \\ split-window -h -c "#{pane_current_path}"

# Copy mode
setw -g mode-keys vi
bind [ copy-mode
bind -T copy-mode-vi v send -X begin-selection
bind -T copy-mode-vi y send -X copy-pipe-and-cancel

# Kill without confirmation
bind x kill-pane

# Reload config
bind r source-file ~/.tmux.conf \; display "Reloaded"
```

**Quick reference:**

| Action | Keys |
|--------|------|
| Navigate panes | `Alt-hjkl` (no prefix) |
| Window 1-5 | `Alt-1` through `Alt-5` |
| Split horizontal | `Ctrl-Space -` |
| Split vertical | `Ctrl-Space \` |
| Close pane | `Ctrl-Space x` |
| Copy mode | `Ctrl-Space [` |
| Zoom pane | `Ctrl-Space z` |

### J.9 Not Required

These are handled by tmux on imladris, not the client terminal:

| Feature | Where |
|---------|-------|
| Session persistence | tmux (server) |
| Splits/panes | tmux (server) |
| Scrollback | tmux (server) |
| Copy mode | tmux (server) |

Kitty provides: image display, keyboard hints, font rendering, input handling.

---

## Appendix K: Feature Backlog

Future features and skills to implement. Organized by priority and category.

### K.1 High Priority

| Feature | Category | Description | Effort |
|---------|----------|-------------|--------|
| **Daily Morning Briefing** | Briefing | Weather, tasks, calendar, Denver news combined | Medium |
| **GCP Skill** | Work Integration | Cloud management, project access | Medium |
| **AWS Spend Analysis** | Work Integration | Cost analysis, budget alerts | Medium |
| **Simple Home Task System** | Personal | Flat-file or SQLite tasks outside SDP | Low |
| **Self-Healing Session Review** | Improvement | Review logs/errors, suggest improvements | Medium |
| **AI/Cloud Release Digest** | Briefing | AWS, OpenAI, Anthropic, Gemini, GCP updates | Low |

### K.2 Medium Priority

| Feature | Category | Description | Effort |
|---------|----------|-------------|--------|
| **SignOz Integration** | Work Integration | Observability platform skill | Medium |
| **Slack Holidays Tool** | Work Integration | Show country holidays, work hours, timezones | Low |
| **Chat Consolidation** | Personal | Route all chats through SimpleX (iMessage, Telegram, Signal, Slack) | High |
| **Spec-Driven Autonomous Agent** | Improvement | Use spec kit to evaluate/build/test autonomously | High |
| **Content Summary** | Briefing | YT transcripts, Substack summarization | Medium |
| **Calendar Review** | Briefing | Daily/weekly calendar review with context | Low |
| **Task Review** | Briefing | SDP + adhoc incomplete task review | Low |
| **Claude Cookbook Integration** | Integration | Integrate patterns and examples | Low |
| **Gemini/OpenAI Hooks** | Integration | Multi-model support hooks | Medium |
| **MCP Library Evaluation** | Integration | Find and evaluate useful MCPs | Low |

### K.3 Work System Integrations

| System | Purpose | Priority |
|--------|---------|----------|
| Ripple | TBD | Medium |
| Site24x7 | Monitoring integration | Medium |
| GetFlywheel | Hosting management | Low |
| Skylight MCP | Calendar display access | Low |
| Slack SDP Integration | Link Slack to ServiceDesk Plus | Medium |
| TF for MS365 | Terraform for MS365 management | Low |

### K.4 Personal Automations

| Feature | Description | Priority |
|---------|-------------|----------|
| **Date Ideas Generator** | Local events + AI suggestions for dates/relationships | Low |
| **Prescription Tracking** | KS pharmacy notifications, requests, consolidation | Low |
| **Grocery Sales Automation** | Track deals at grocery stores | Low |
| **Media Download Automation** | Automate movie/music acquisition | Low |
| **Entertainment Releases** | Music and movie release tracking | Low |
| **Financial Review** | Daily spend and financial summary | Medium |
| **Home Assistant Skill** | Home automation integration | Low |
| **Weather Skill** | Local weather with Denver front range context | Low |

### K.5 Infrastructure / Reliability

| Feature | Description | Priority |
|---------|-------------|----------|
| **Temp Agent Deploy** | Use Tailscale to quickly deploy temporary agents | Medium |
| **Data Loss Prevention** | Better assurance no work is lost during sessions | Medium |
| **Auto-Fix Testing** | Self-healing with automated tests to verify fixes | High |
| **Skill Evolution** | AI creates/improves skills autonomously over time | Medium |
| **Namespace Simplification** | Cleaner home/work context separation | Low |

### K.6 Philosophy / Methodology

| Topic | Notes |
|-------|-------|
| Toyota Production System | Apply lean principles to workflows |
| Real-time System Principles | Guarantees, guardrails, reproducibility |
| UNIX Elegance | Simple tools that compose well |
| Modularization | Evaluate what should be modular vs integrated |

### K.7 Naming Backlog

| Item | Proposed Name | Context |
|------|---------------|---------|
| Libretto alternative | LOTR-themed name | TBD |

### K.8 Questions to Resolve

| Question | Context |
|----------|---------|
| What is Ripple? | Work system needing integration |
| GetFlywheel scope? | WordPress hosting management |
| KS Prescriptions? | Kaiser? King Soopers pharmacy? |
| Skylight? | Calendar display device? |
| More agnostic automation? | Is there a universal automation approach? |
